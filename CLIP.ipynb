{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\IEmbed}{{\\mathcal{I}}}\n",
    "\\newcommand{\\TEmbed}{{\\mathcal{T}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text and Images together\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Text is represented as\n",
    "            <ul>\n",
    "                <li>a sequence of integers: an integer index into the list of tokens in the vocabulary</li>\n",
    "            </ul>\n",
    "        <li>Using the ideas in the VQ-VAE, an image can be represented as\n",
    "            <ul>\n",
    "                <li>a sequence of integers: an integer index into a \"code book\": finite list of vectors</li>\n",
    "            </ul>\n",
    "        <li>Both text and image sequences can be reduced to a fixed-length vector: an embedding\n",
    "        <li>Goal:\n",
    "        <ul>\n",
    "            <li>Create a shared embedding space for text and images</li>\n",
    "            <li>Learn projection matrices\n",
    "                <ul>\n",
    "                    <li>One to project the text embedding into the shared space</li>\n",
    "                    <li>One to project the image embedding into the shared space</li>\n",
    "                </ul>\n",
    "            <li>Such that the projections\n",
    "                <ul>\n",
    "                    <li>Of an image</li>\n",
    "                    <li>And a text string that describes the image</li>\n",
    "                    <li>Are close to one another in the shared space\n",
    "                </ul>\n",
    "            <li>and the projections</li>\n",
    "                <ul>\n",
    "                    <li>Of an image</li>\n",
    "                    <li>And a text string that <b>does not</b> describes the image</li>\n",
    "                    <li>Are <b>not close</b> to one another in the shared space\n",
    "                </ul>\n",
    "        </ul>\n",
    "        <li>By creating text and image embeddings with this \"closeness\" property</li>\n",
    "            <ul>\n",
    "                <li>We can perform zero-shot Image Classification</li>\n",
    "                <li>Classify images by matching them against sentences describing a possible class: \"Photo of a cat\"</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**References**\n",
    "- [CLIP paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\n",
    "- [LiT paper](https://arxiv.org/pdf/2111.07991.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this module, we show how to mix Text and Images\n",
    "- **outside** the context of an LLM\n",
    "    - no tokenization\n",
    "    \n",
    "This is useful for tasks that are different than the Language Model task (\"predict the next token\").\n",
    "\n",
    "In particular, we show how to create an Image Classifier\n",
    "- Image input, text output\n",
    "    - output is a label or a caption of the input image\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The standard \"Computer Vision\" task is to learn an association between images and a predetermined set of labels.\n",
    "\n",
    "Highly successful image classifiers have thus been obtained.\n",
    "\n",
    "**But** they are directly useful only for images from the same distribution as the Training dataset\n",
    "\n",
    "Transfer Learning is necessary to adapt the Image Classifier to images that are different than training\n",
    "- e.g.,different labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The traditional Classifier architecture \n",
    "- assigns disjoint roles for the two types of data\n",
    "    - Images are Features\n",
    "    - Text are Labels\n",
    "- Can't learn to associate (parts of) Images with semantically related Text\n",
    "- usually requires hand-labeled Images\n",
    "    - expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We introduce a model called CLIP (Contrastive Image Language-Image Pre-training) \n",
    "- that treats Image and Text in a common role\n",
    "\n",
    "This, in turn, will facilitate the creation of an Image Classifier\n",
    "- **without** Transfer Learning/Fine-Tuning to a new Target domain\n",
    "\n",
    "The result is\n",
    "- Zero-shot Image Classification of images *distinct from a training dataset*\n",
    "\n",
    "And\n",
    "- it is trained on an *abundant* source of image/text pairs\n",
    "    - no expensive construction of a hand-labeled training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, we highlight the Loss Function\n",
    "- Contrastive Loss\n",
    "\n",
    "that illustrates how the Loss functions of the Advanced course are more\n",
    "complex than the simple ones of the Intro course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A shared embedding space for Image and Text\n",
    "\n",
    "## Image embedding space\n",
    "\n",
    "A traditional Image Classification model consists of\n",
    "- multiple layers prior to the \"Classification head\"\n",
    "    - that creates alternate representations of the Input image\n",
    "    - of increasing complexity as we go Deeper (closer to the Classification Head)\n",
    "   \n",
    "The purpose of the layers from the Input to the Classification Head\n",
    "- is to create a representation\n",
    "- from which the Classification Head can correctly predict the label of an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Transfer Learning: replace the head, deep layers of the pre-trained model</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/Transfer_Learning_3.jpg\"></center></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We call the alternative representation of an Image\n",
    "- an *Image Embedding*\n",
    "\n",
    "*Transfer Learning* (illustrated above)\n",
    "- uses the Image Embedding of some Deep Layer of a Source Task Image Classifier\n",
    "- to *transfer* properties that is common to \n",
    "    - the Source Task Images\n",
    "    - the Target Task Images\n",
    "- in order to solve a new Target Task\n",
    "\n",
    "So an Embedding captures properties of generalized Images\n",
    "- independent of the Source and Target task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The traditional Image Classifier\n",
    "- learns an association between an Image and Text\n",
    "\n",
    "**But** this is association is very shallow\n",
    "- no **meaning** is associated with the text labels\n",
    "\n",
    "This is because the text labels\n",
    "- are OHE\n",
    "    - mutually orthogonal\n",
    "- so no semantic association is possible between\n",
    "    - different labels/different images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text embedding space\n",
    "\n",
    "\"Language Models\" (\"predict the next token\") have demonstrated a great ability for creating representations of text sequences\n",
    "- Seem to capture semantics\n",
    "- The fixed length \"summary\" of a text sequence is a *text embedding*\n",
    "- Facilitate zero shot learning\n",
    "    - Universal \"text to text\" API for all language tasks\n",
    "    - Single model can \"learn\" to solve a new task without adjustment of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>Latent state $\\h_\\tp$ is a fixed length \"summary\" of $\\x_{(1)}, \\ldots \\x_\\tp$</strong></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><img src=\"images/RNN_many_to_one_to_classifier.jpg\"></td></center>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We call the fixed length vector representation of a Text sequence (alternative representation)\n",
    "-a *Text embedding*\n",
    "\n",
    "These embeddings capture a lot of semantic concepts of Text\n",
    "- in contrast to the Image Classifier's lack of \"understanding\" of the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a common embedding space\n",
    "\n",
    "We can try to use\n",
    "- Image embeddings\n",
    "- Text embeddings\n",
    "\n",
    "together in a single model\n",
    "- to create an Image Classifier\n",
    "- with a deeper association between Images and layer\n",
    "    - compared to the traditional Image Classifier\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One problem with this idea\n",
    "- the Image and Text embedding spaces\n",
    "- are disjoint\n",
    "    - trained separately\n",
    "    - maybe even the lengths of the embedding vectors differ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "CLIP creates a *joint embedding space* for Image and Text.\n",
    "- The embedding space of Images\n",
    "- The embedding space of Text\n",
    "- are combined into a *common* embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given\n",
    "- an Image Encoder that embeds image $I$ into an Image Embedding $I_f$ \n",
    "$$I_f = \\text{image_encoder}(I)$$\n",
    "- a Text Encoder $T_f$ embedding text sequence $T$ into a Text Embedding $T_f$\n",
    "$$T_f = \\text{text_encoder}(T)$$\n",
    "\n",
    "CLIP defines two matrices that create a *common* embedding space\n",
    "- matrix $W_i$ to project from Image Embedding space to common space\n",
    "- matrix $W_t$ to project from Text Embedding space to common space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Image and Text embeddings in the common space\n",
    "- Image embedding $I_f$ projected into common  space\n",
    "\n",
    "$$I_e = I_f \\cdot W_i$$\n",
    "\n",
    "- Text embedding $I_t$ projected into common  space\n",
    "\n",
    "$$T_e = T_f \\cdot W_t$$\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "CLIP trains a model using training data examples that map Images to Text\n",
    "- just like any other Image Classifier\n",
    "\n",
    "For a training example associating text sequence $T$ (label, caption) with image $i$\n",
    "$$\n",
    "\\langle I, T \\rangle\n",
    "$$\n",
    "\n",
    "CLIP training definesmatrices $W_i,W_t$ \n",
    "\n",
    "- that *maximize the similarity* of the embeddings of $I$ and $T$ in the common space\n",
    "\n",
    "$$\n",
    "I_e \\cdot T_e\n",
    "$$\n",
    "\n",
    "That is\n",
    "- the embedding of the Image and its associated Text\n",
    "- are close in common embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CLIP loss: Contrastive Learning\n",
    "\n",
    "The loss function and training for CLIP is of particular interest.\n",
    "\n",
    "An important part of the Model is the *Contrastive Training* objective\n",
    "- Minimize the distance between *correct* Image/Text Pairs\n",
    "- Maximize the distance between *incorrect* Image/Text pairs\n",
    "\n",
    "This type of objective is used in many places in Deep Learning\n",
    "- so is worthy to  introduce as an independent concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zero shot Image Classification\n",
    "\n",
    "With a common Image/Text embedding space, Image Classification is easy\n",
    "- does not require further training for a type of Image not seen in training\n",
    "\n",
    "Simply\n",
    "- embed all the Target labels into common embedding space\n",
    "- given an Image $I$ at inference time\n",
    "    - embed the image in common embedding space: $I_e$\n",
    "    - compute the dot product of $I_e$ with the embedding of each Target label\n",
    "    - select the Target label $T$ with greatest similarity\n",
    "    $$ I_e \\cdot T_e$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "**Notation summary**\n",
    "\n",
    "term | dimension &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | meaning \n",
    ":---|:---|:---\n",
    "$n$ | | number of examples in a training batch\n",
    "$N$ | | number of examples in the training dataset\n",
    "$I$ | $(n \\times h \\times w \\times c )$ | Image batch\n",
    "| | image dimension $(h \\times w \\times c )$\n",
    "$T$ | $(n \\times l )$ | Text labels for batch\n",
    "$d_i$ | | dimension of Image embedding\n",
    "$d_t$ | | dimension of Text embedding\n",
    "$d_e$ | | dimension of common embedding\n",
    "$W_i$ | $(d_i \\times d_e)$ | learned projection of image embedding to common embedding\n",
    "$W_t$ | $(d_t \\times d_e)$ | learned projection of image embedding to common embedding\n",
    "$t$   | | learned temperature parameter (used in softmax)\n",
    "$I_f$ | $(n \\times d_i)$ | embedding of Image batch\n",
    "$T_f$ | $(n \\times d_t)$ | embedding of Text (label) batch\n",
    "      | $(n \\times d_e)$ | size of common embedding (for each of the $n$ Image and Labels)\n",
    "$\\text{logits}$ | $(n \\times n)$ | $\\text{logits}_{i,j}$ is similarity of image $i$ to label $j$      \n",
    "$\\loss_i$ | $1$ | Loss across images: reduce $(\\text{logits} \\cdot \\text{labels})$ across text dimension\n",
    "           |    | For each image: compare image's logits to labels\n",
    "$\\loss_t$ | $1$ | Loss across labels: reduce $(\\text{logits} \\cdot \\text{labels})$ across image dimension\n",
    "           |    | For each label: compare labels's logits to images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrastive pre-training\n",
    "\n",
    "CLIP is trained to solve an Image Classification task\n",
    "- Associate the best text sequence description\n",
    "- To an input Image\n",
    "\n",
    "The \"standard\" approach is to jointly train\n",
    "- an image feature extractor (e.g., a CNN)\n",
    "- with a Classifier \"head\"\n",
    "    - the labels are sparse OHE vectors representing \"word\" labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast, CLIP jointly trains\n",
    "- an Image encoder\n",
    "- a Text sequence encoder\n",
    "- with the objective of\n",
    "    - matching a Training image with its associated Text\n",
    "    - the text is a semantically meaningful sequence of \"words\" rather than a OHE vector\n",
    "\n",
    "The key is for the Image Encoder and Text Encoder to produce embeddings in a common space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method of *Contrastive Learning* \n",
    "- is to learn features (Image/Text embeddings in the common space)\n",
    "- that make related concepts (an Image and correct Text) \"similar\"\n",
    "    - high similarity, low distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hopefully, the picture (labeled \"(1)\") describing the *training* will help:\n",
    "- A training example is an (Image, Text) pair: $(I_i, T_i)$\n",
    "- The Image $I_i$ is encoded into the joint embedding space by the Image Encoder\n",
    "- It's Text $T_i$ is encoded into the joint embedding space by the Text Encoder\n",
    "- The matrix is a \"similarity\" (inverse of distance) between all Images and all Text labels\n",
    "    - the similarity is defined by the dot product of the Image and Text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><strong>CLIP architecture</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contrastive training details\n",
    "\n",
    "\n",
    "## Examples\n",
    "\n",
    "Given a set of labeled training examples\n",
    "\n",
    "$$\n",
    "\\langle I_i, T_i \\rangle \\; \\text{ for } 1 \\le i \\le m\n",
    "$$\n",
    "\n",
    "Choose\n",
    "- an Image Encoder: \n",
    "    - $d_i$ is the length of image embeddings\n",
    "- a Text Encoder: \n",
    "    - $d_t$ is the length of text embeddings\n",
    "\n",
    "each creating separate embedding spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model parameters\n",
    "\n",
    "Define  learnable model parameters\n",
    "- matrix $W_i, W_t$\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "W_i: & (d_i \\times d_e) \\\\\n",
    "W_t: & (d_t \\times d_e) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "These matrices enable project of each example image $I_i$ and label $T_i$ into common space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Similarity matrix \n",
    "\n",
    "Create a similarity matrix $S$ \n",
    "\n",
    "$$\n",
    "S: (m \\times m)\n",
    "$$\n",
    "\n",
    "that compares each image in the training data set with each label in the training data set.\n",
    "\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "S^\\ip & \\text{Row } i    & \\text{comparison of image } I_i \\\\\n",
    "S_j   & \\text{Column } j & \\text{comparison of Text label } T_j \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S^\\ip_j = \\IEmbed_i \\cdot \\TEmbed_j\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\IEmbed_i, \\TEmbed_j$ are the embeddings *in common space* of\n",
    "- Image $I_i$\n",
    "- Text label $T_j$\n",
    "\n",
    "This is the matrix on the left side of the diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than using the intensity of the dot product\n",
    "- we convert to probabilities\n",
    "- using the Softmax on each row\n",
    "\n",
    "when defining each of the components of the Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doing so enables us to create\n",
    "- a probability distribution $p^\\ip$\n",
    "- using row $S^\\ip$ \n",
    "- over Text labels $\\{ T_j \\; | \\; 1 \\le j \\le m \\}$\n",
    "\n",
    "representing the probability that image $I_i$ is each of the possible labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and to create \n",
    "- a probability distribution $p_j$\n",
    "- using column $S_j$\n",
    "- over Images $\\{ I_i \\; | \\; 1 \\le i \\le m \\}$\n",
    "\n",
    "representing the probability that Text $T_j$ corresponds to each possible Image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contrastive Loss function\n",
    "\n",
    "Suppose we have a single (Image, Text) pair $(I_i, T_i)$ from the set of $m$ training examples.\n",
    "\n",
    "Our objective is to ensure that the similarity  in common space is such that\n",
    "- $\\IEmbed_i \\cdot \\TEmbed_i$ is as large as possible\n",
    "- $\\IEmbed_i \\cdot \\TEmbed_j$ is as small as possible for $i \\ne j$\n",
    "\n",
    "That is: we maximize the *contrast* between\n",
    "- an image and its correct label\n",
    "- and the image with any incorrect label\n",
    "\n",
    "We define two Losses\n",
    "over the entries of Similarity Matrix (probability form) $S$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "We create an *Image Loss* for example $i: \\loss_i^\\ip$.\n",
    "\n",
    "Recall that $p^\\ip$ is the probability distribution over Text labels.\n",
    "- for Image $I_i$\n",
    "\n",
    "Entry $i$ in this distribution vector is the entry for the *correct* label $T_i$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define \n",
    "- target probability vector $t^\\ip$ for example $i$\n",
    "- as the OHE vector \n",
    "    - where $t^\\ip_i = 1$ is the only non-zero entry\n",
    "\n",
    "Minimizing the Cross Entropy loss of $p^\\ip$ and $t^\\ip$\n",
    "- states the goal of having \n",
    "    - $p^\\ip_i$ as close to $1$ as possible\n",
    "    - $p^\\ip_j$ as close to $0$ as possible for $j \\ne i$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can similarly  define a *Text Loss* for example $i: \\loss_t^\\ip$.\n",
    "\n",
    "Recall that  $p_i$ is the probability distribution over Images\n",
    " - for Text label $T_i$\n",
    " \n",
    "Entry $i$ in this distribution vector is the entry for the *correct* image $I_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As above\n",
    "- we create Text Loss for example $i$ using Cross Entropy.\n",
    "\n",
    "Minimizing Cross Entropy loss of $p_i$ and $t_i$\n",
    "- states the goal of having\n",
    "    - $p^\\ip_i$ as close to $1$ as possible\n",
    "    - $p^{(j)}$ as close to $0$ as possible for $j \\ne i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Total loss\n",
    "\n",
    "The per example loss is the sum of the per example Image and Text losses\n",
    "$$\n",
    "\\loss^\\ip = \\loss_i^\\ip + \\loss_t^\\ip\n",
    "$$\n",
    "\n",
    "and the Total Loss is the sum over the $m$ per example losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating Contrastive examples\n",
    "\n",
    "With a large number of examples $m$\n",
    "- the similarity matrix becomes large\n",
    "\n",
    "There is a clever trick to avoid computing the large matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Essentially\n",
    "- the per-example loss $\\loss^\\ip$ for example $i$\n",
    "- involves the comparison over all $m$ examples\n",
    "\n",
    "Recall that mini-batch Gradient Descent is used in training Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than comparing example $i$ against\n",
    "- all $m$ training examples \n",
    "- we compare it against\n",
    "    - only the examples in the *same mini-batch* as example $i$\n",
    "\n",
    "To simplify the computation further\n",
    "- the target $t^\\ip$ (used in minimizing the Cross Entropy of $p^\\ip$)\n",
    "- is constructed under the (imperfect) assumption\n",
    "- that all examples $j \\ne i$ in the mini-batch have labels that are different than true label $T_i$\n",
    "\n",
    "This is called the *in-batch negatives* trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The in-batch negatives trick is simple but crude.\n",
    "- a \"second best\" label\n",
    "- is considered equally bad\n",
    "- as a \"completely incorrect\" label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pseudo-code for Pre-Training \n",
    "\n",
    "Pseudo code for training batch:\n",
    "```\n",
    "I_f = image_encoder(I)\n",
    "T_f = text_encoder(T)\n",
    "\n",
    "# joint multimodal embedding [n, d_e]\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\n",
    "\n",
    "# scaled pairwise cosine similarities [n, n]\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\n",
    "\n",
    "# symmetric loss function\n",
    "labels = np.arange(n)\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "\n",
    "loss = (loss_i + loss_t)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-training dataset\n",
    "\n",
    "In order to train CLIP successfully\n",
    "- we need a large number of Image/Text pairs.\n",
    "\n",
    "Other than manual labeling: where do these come from ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The authors adopt a similar approach to obtaining training data for Image Classification\n",
    "- Search the Web for Images that have captions\n",
    "- The caption become the labels for the training image\n",
    "- **Unbounded** number of distinct labels\n",
    "    - many Texts to describe the same (or similar image)\n",
    "        - \"Picture of a cat\"\n",
    "        - \"Picture of my cat named 'Kitty'\"\n",
    "        - unbounded length of label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is one reason Text labels need to be embedded\n",
    "- Need a fixed length representation an unbounded number of labels, each of unbounded length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference: Zero shot\n",
    "\n",
    "The fact that CLIP can deal with arbitrary labels (in the force of Text sequences) creates the possibility of classifying Images\n",
    "- from an *unseen* Target dataset\n",
    "- with no further training (other than the initial pre-training on the Source dataset)\n",
    "\n",
    "Being able to solve a Target task without specifically being trained with examples of the task\n",
    "- is called **Zero shot** learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The pictures (labeled \"(2)\" and \"(3)\") in the CLIP diagram above describe the process\n",
    "for predicting the label of a single image\n",
    "- Given the finite set of labels from the Target Task\n",
    "- Convert the short labels (nouns or phrases) into longer sentences\n",
    "    - i.e., \"Cat\" becomes \"photo of a cat\"\n",
    "- Embed the sentences into joint embedding space,  resulting in $T_1, \\ldots T_N$\n",
    "- Embed the Target Image into joint embedding space, resulting in $I_1$\n",
    "- Create the similarity matrix of dimension $(1 \\times N)$\n",
    "    - computing $I_1 \\cdot T_j$, for each $1 \\le j \\le N$\n",
    "- Predict $j^* = \\argmax{j}{I_1 \\cdot T_j}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "## Prompt Engineering\n",
    "\n",
    "In the description of zero shot inference, short labels of the Target task were expanded into longer sequences of words.\n",
    "\n",
    "The authors call this *prompt engineering* (i.e., creating new \"prompts\" for input.\n",
    "\n",
    "They suggest that careful prompt engineering for each Target task can improve Zero shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- converting a label (denoted by placeholder `{label}`) for a pet  to\n",
    " > A photo of a {label}, a type of pet\n",
    "- can improve classification\n",
    "    - helps with polysemy (two words with identical spelling but different meaning)\n",
    "        - \"crane\": a bird; a piece of construction equipment\n",
    "    - the extra words \"photo\" and \"type of pet\" become attributes of the image\n",
    "        - that can be related (by textual similarity) to other images/labels through the Text embeddings\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Theory**\n",
    "\n",
    "Representation of CLIP is much more detailed than other Image Classification models\n",
    "- Other models only need to find representation that separates examples\n",
    "    - may be hyper-specific and not generalize well\n",
    "- CLIP needs to understand other details of the image **through the text**\n",
    "    - difference between image formats: \"photo\", \"illustration\", \"drawing\"\n",
    "    - sub-images that are mixed with the target sub-image\n",
    "        - a \"cat\" in a group of \"cats\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Zero shot classification with Prompt Engineering\n",
    "\n",
    "Let's explore Prompt Engineering using this [Colab notebook](https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb)\n",
    "\n",
    "Goal is zero-shot classification of CIFAR image dataset\n",
    "- 1000 classes\n",
    "- most class labels are single word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Feature engineering transformed each CIFAR class label into *multiple* prompts\n",
    "using \"templates\"\n",
    "```\n",
    "imagenet_templates = [\n",
    "    'a bad photo of a {}.',\n",
    "    'a photo of many {}.',\n",
    "    'a sculpture of a {}.',\n",
    "    'a photo of the hard to see {}.',\n",
    "    'a low resolution photo of the {}.',\n",
    "    'a rendering of a {}.',\n",
    "    'graffiti of a {}.',\n",
    "    'a bad photo of the {}.',\n",
    "    'a cropped photo of the {}.',\n",
    "    'a tattoo of a {}.',\n",
    "    'the embroidered {}.',\n",
    "    'a photo of a hard to see {}.',\n",
    "    'a bright photo of a {}.',\n",
    "    'a photo of a clean {}.',\n",
    "    'a photo of a dirty {}.',\n",
    "    'a dark photo of the {}.',\n",
    "    'a drawing of a {}.',\n",
    "    'a photo of my {}.',\n",
    "    'the plastic {}.',\n",
    "```\n",
    "$$\\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using forward selection, the authors selected the 7 \"best templates\"\n",
    "- Created 7 prompts (e.g., texts) from each CIFAR class label\n",
    "- Tokenized each prompt\n",
    "- text-encoded each to the common embedding\n",
    "- normalized each embedding\n",
    "- took the average (across the 7 prompts ?) embedding\n",
    "    - and used it as the embedding for the CIFAR class\n",
    "\n",
    "```\n",
    "texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Eventually: the image embedding has its cosine similarity compared with the average embedding for each label\n",
    "\n",
    "```\n",
    "# predict\n",
    "image_features = model.encode_image(images)\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "logits = 100. * image_features @ zeroshot_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observation from notebook on why templates help\n",
    "> Speculating, we think it's interesting to see different scales (large and small), a difficult view (a bad photo), and \"abstract\" versions (origami, video game, art), were all selected for, but we haven't studied this in any detail. This subset performs a bit better than the full 80 ensemble reported in the paper, especially for the smaller models.\n",
    "\n",
    "Adding textual hints (via prompt engineering: \"a bad photo\") seemed to help\n",
    "- perhaps implicitly creating an attribute of an image\n",
    "- that creates a relationship with other Images having a similar attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments\n",
    "\n",
    "The authors report many experiments using CLIP, in an effort to discover\n",
    "- its strengths, weaknesses\n",
    "- how it works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One experiment compares\n",
    "- Zero shot learning of a Target task, using CLIP\n",
    "- Transfer learning\n",
    "    - creating a Target task specific head on top of an existing Source task Image Classifier (e.g., ResNet)\n",
    "    \n",
    "They call the Transfer Learning method *linear probing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Zero shot CLIP outperforms Linear Probing with ResNet on many classification tasks\n",
    "- does better on Target tasks in which Target task training set has few examples per class\n",
    "    - i.e., too few examples per class to adequately train the new Classification head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias\n",
    "\n",
    "Section 7 of the paper is an effort to probe implicit biases that we\n",
    "have learned are present\n",
    "in seemingly unbiased text (e.g., Wikipedia)\n",
    "\n",
    "For example, there are datasets used to probe for biases about race\n",
    "- add \"egregious\" categories: animals (\"ape\", \"orangutan\"), criminal (\"thief\") to true Text label\n",
    "- Blacks misclassified as animals more often than Whites\n",
    "- Young people misclassified more often as thief\n",
    "    - but adding a \"child\" class reduces this misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Understanding that a model learns unintended bias \n",
    "- through biased natural language\n",
    "- through photos in very limited contexts\n",
    "\n",
    "is something that is becoming more prevalent in describing and evaluating models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Similar approach from Keras site\n",
    "\n",
    "There is a similar [example](https://keras.io/examples/nlp/nl_image_search/) on the Keras website.\n",
    "\n",
    "It's instructive to compare the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Joint Embedding**\n",
    "\n",
    "CLIP: via a shared learned embedding matrices $\\W_i$ (image) and $\\W_t$ (text).\n",
    "\n",
    "Keras: via two separate multi-layer \"embedding\" networks of blocks consisting of `Dense` layers to transform the dimension.\n",
    "\n",
    "```\n",
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings\n",
    "```\n",
    "\n",
    "Note that the statement\n",
    "\n",
    "    x = layers.Add()([projected_embeddings, x])\n",
    "\n",
    "is implementing a skip connection.\n",
    "- `projected_embeddings` that is input to the body of the loop\n",
    "- skips over the body\n",
    "- and is added to the updated (by the body) `projected_embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Loss function**\n",
    "\n",
    "The \"logits\" are the dot-product of the Text and Image embeddings.\n",
    "\n",
    "CLIP: Compare the logits to the text target labels and (separately) to the image target labels.\n",
    "- recall: the target \"labels\" are just indices since the diagonal element is the correct \"target\"\n",
    "    \n",
    "    loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "    loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "\n",
    "    loss = (loss_i + loss_t)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Keras\n",
    "\n",
    "This approach is a little more sophisticated.\n",
    "\n",
    "It compares the similarity (using dot product) of\n",
    "- pairs of labels\n",
    "- pairs of images\n",
    "\n",
    "It uses the average similarity as the \"correct\" label\n",
    "- the best target (highest average) is the correct one: $I_i, T_i$\n",
    "- but mis-classifying a text/image pair is mitigated if the incorrect class\n",
    "    - have similar texts\n",
    "    - have similar images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CLiP demo (HuggingFace)\n",
    "\n",
    "You can use the Inference API to play with [CLiP on HuggingFace](https://huggingface.co/openai/clip-vit-large-patch14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
