{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\rewmod}{\\mathbb{r}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$\n",
    "\\newcommand{\\rewmod}{\\mathbb{r}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classic approach: PPO with Preferences instead of Rewards\n",
    "\n",
    "Before preference-oriented methods were introduced\n",
    "- PPO was the main method for RL with preferences\n",
    "- It still is \n",
    "\n",
    "But it is complex and costly.\n",
    "- multiple steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The steps are\n",
    "- translate preferences to rewards\n",
    "- run PPO on the translated rewards in the usual manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Reward Model\n",
    "\n",
    "It is difficult and manually-intensive for a human to translate preferences to rewards\n",
    "- Need to be consistent across examples\n",
    "- No guiding principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead, we train a *Reward Model* $\\rewmod_\\phi$\n",
    "- Neural Network\n",
    "- parameterized by $\\phi$\n",
    "- satisfying\n",
    "\n",
    "$$\n",
    "\\rewmod_\\phi(x, y^+) > \\rewmod_\\phi(x, y^-)\n",
    "$$\n",
    "\n",
    "for all preferences\n",
    "$$\n",
    "(x, y^+, y^-)\n",
    "$$\n",
    "\n",
    "using per-example Loss function\n",
    "$$\n",
    "\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-) = - \\log \\sigma \\big(\n",
    "\\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss function\n",
    "- maximizes the spread between the reward for the preferred and non-preferred output\n",
    "- converts it, via the sigmoid $\\sigma$ function into the range $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model is trained on a set of Preference Data\n",
    "- human preferences\n",
    "    - costly\n",
    "- LLM \"Judge\" preferences\n",
    "\n",
    "The training causes the reward model to\n",
    "imitate the preferences inherent in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward model: training\n",
    "\n",
    "- A prompt (context) is fed to the both a human (offline) and the model\n",
    "- The model creates model responses (continuation)\n",
    "- The Reward Model and the Human both rank the responses (calculate a reward)\n",
    "- The Loss function penalizes the model for model rewards that deviate from the human reward\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <img src=\"images/instructgpt_reward.png\" width=75%>\n",
    "    </tr> \n",
    "    <br>\n",
    "     <tr> \n",
    "         <center>context = prompt; continuation = response</center>\n",
    "    </tr>\n",
    "    <br><br>\n",
    "    <tr>\n",
    "    Source: https://arxiv.org/pdf/1909.08593.pdf#page=2\n",
    "    </tr>\n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion: Reward model\n",
    "\n",
    "\n",
    "Typically\n",
    "- the Reward Model is a scaled-down version of the Policy Model\n",
    "- which may still have a large number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**$\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-)$ interpretation**\n",
    "\n",
    "There is also an interesting interpretation of the per-example Loss\n",
    "$$\n",
    "\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-) = - \\log \\sigma \\big(\n",
    "\\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "In Logistic Regression\n",
    "- we compute a score $z$\n",
    "- convert $z$ to a probability $\\sigma(z)$ \n",
    "    - probability of \"example being Positive\" via the sigmoid function\n",
    "- use Binary Cross Entropy as the Loss\n",
    "    - sum of terms for \n",
    "        - Positive examples: $ - \\log( \\sigma(z) )$ \n",
    "        - Negative examples: $ \\log (1 - \\sigma(z))$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our case: the triples are all \"Positive\" examples so cross entropy collapses to $ - \\log( \\sigma(z) )$ \n",
    "\n",
    "So we can view \n",
    "$$\n",
    "\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-) = - \\log \\sigma \\big(\n",
    "\\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "as the Cross Entropy Loss of predicting the probability\n",
    "$$\n",
    "\\prc{y^+ \\text{ preferred to } y^-}{x}\n",
    "$$\n",
    "where the score $z$ is\n",
    "$$\n",
    "z = \\rewmod_\\phi(x, y^+) - \\rewmod_\\phi(x, y^-)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The PPO model\n",
    "\n",
    "Once example preferences are translated into rewards\n",
    "- example $(x,y)$ is assigned reward \n",
    "$$\\rewmod_\\phi(x, y)$$\n",
    "- we apply classic PPO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "PPO for Preferences involves one model\n",
    "- Reward Model\n",
    "\n",
    "**in addition to** the models inherent in PPO\n",
    "- policy\n",
    "- reference\n",
    "- Value/Critic: for advantage computation\n",
    "\n",
    "| Model Type   | Typical Size                           | Role                          |\n",
    "|:--------------|:---------------------------------------|:-------------------------------|\n",
    "| Policy Model | Large-scale pretrained LM (billions) | Generate responses            |\n",
    "| Reward Model | Smaller/fine-tuned LM or distilled (hundreds of millions) | Score outputs based on preferences |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Group Relative Policy Optimization (GRPO)\n",
    "\n",
    "GRPO is a policy-based RL method for dealing with Preference Data.\n",
    "\n",
    "It thus fits nicely within (a slightly simplified) form of our Unified Policy Gradient Formulation\n",
    "\n",
    "$$\\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \n",
    "\\nabla_\\theta J(\\theta) = \\sum_{\\tt=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(\\actseq_{\\tau,\\tt} | \\stateseq_{\\tau,t}) A_{\\tau,\\tt} \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In GRPO, the Advantage of a response is defined\n",
    "- **relative** to a **group** of responses to the **same** prompt\n",
    "- rather than an *absolute* advantage\n",
    "\n",
    "The Advantage of two responses in different groups\n",
    "- are on a similar scale\n",
    "- all prompts (and their group of responses) have advantages on same scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Advantage definition**\n",
    "\n",
    "Given a prompt/input $x$\n",
    "- a group $g = \\rm{group}(x)$ of size $G$ sample responses are collected\n",
    "\n",
    "\n",
    "For each sample response $y_i \\in g$ \n",
    "- there is a corresponding reward $\\rewseq(g,  y_i)$\n",
    "\n",
    "The rewards within group $g$ are normalized, giving the advantage for $y_i$ as\n",
    "$$\n",
    "A_{g,y_i} = \\frac{\\rewseq(g,  y_i) - \\mu_g}{\\sigma_g}\n",
    "$$\n",
    "\n",
    "where $(\\mu_g, \\sigma_g)$ are the mean and standard deviation of the rewards withing group $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Specializing Unified Policy Gradient Formulation to LLM Next Token prediction task**\n",
    "\n",
    "We need to simplify the formulation to deal with Preference data.\n",
    "\n",
    "We do this specifically within the context of \n",
    "- an LLM\n",
    "- performing Language Modeling\n",
    "    - Predict the Next Token conditional on the previously generated tokens of the response\n",
    "- producing output $y$ given input $x$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A response $y$ is thus a sequence of tokens.\n",
    "\n",
    "We turn the LLM's `generate` loop into an RL formulation as follows.\n",
    "\n",
    "Identify iteration $\\tt$ of the LLM's `generate` loop\n",
    "- with a state $\\stateseq_\\tt$ that has produced the first $(\\tt-1)$ tokens of response $y$\n",
    "$$\n",
    "y_{[0:\\tt-1]}\n",
    "$$\n",
    "- and performs action $\\actseq_\\tt$\n",
    "\n",
    "    append next token $y_\\tt$ to the sequence $y_{[0:\\tt-1]}$\n",
    "\n",
    "with probability $\\pi_\\theta( \\actseq_\\tt | \\stateseq_\\tt )$ being defined by the LLM next-token output probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write the probability $\\pi_\\theta( y | x ) $ of the trajectory of  response $y$ given input $x$ as \n",
    "- the chained probabilities of each action given a state\n",
    "\n",
    "The trajectory probability will suffice for re-writing the Unified Policy Gradient Formulation\n",
    "- and can be expanded into the chained probability form, if desired\n",
    "- but is not necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simplified Policy Gradient Formulation is\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) =  \\mathbb{E}_{x} \\left[  \\sum_{y \\in \\rm{group}(x)} \\nabla_\\theta \\log \\pi_\\theta(y | x) A_{\\rm{group}(x),y} \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "where\n",
    "- $x$ is an input to the LLM\n",
    "- $\\rm{group}(x)$ is a set of LLM outputs, given $x$ as input\n",
    "    - since the LLM actions are probabilistic\n",
    "- the advantage $A_{\\rm{group}(x),y}$\n",
    "    - of a particular response $y \\in  \\rm{group}(x)$  \n",
    "    - is *relative* to other members of $\\rm{group}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "The Surrogate Loss for GRPO and PPO\n",
    "- are similar in form\n",
    "\n",
    "But GRPO is a major simplification over PPO\n",
    "- *eliminates* the need for a Value function\n",
    "    - using trajectory-level rewards\n",
    "    - compared to token-level rewards for PPO (derived from the Value function)\n",
    "\n",
    "GRPO differs from PPO \n",
    "- adds a KL-constraint to limit the policy update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Group-relative, normalized Advantage**\n",
    "\n",
    "- **Normalization**\n",
    "\n",
    "The advantage of a response $y$ given input $x$\n",
    "- is  relative to alternate responses to the same inputs $x$\n",
    "    - in units of \"number of standard deviations of the group\"\n",
    "- across groups:\n",
    "    - there is a different standard deviation per group\n",
    "    - but the response to two different inputs $x, x'$ (and hence groups $g, g'$) are in similar units\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So a group $g'$ with rewards that are much larger in scale than the rewards in group $g$\n",
    "- has advantages that are on the same scale as $g$\n",
    "\n",
    "The absence of an absolute reward means\n",
    "- we can pool responses to different inputs \n",
    "- without rewards from group $g'$ over-shadowing rewards from group $g$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, normalization to mean $0$ and  unit standard deviation\n",
    "- reduces variance of gradients\n",
    "- smoother parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Group-relative**\n",
    "\n",
    "Relative (to the other responses in the group) advantage moves the focus to\n",
    "within-group preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Groups**\n",
    "\n",
    "Having multiple responses per prompt $x$\n",
    "- provides multiple updates per prompt, rather than just a single response $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo code for GRPO\n",
    "\n",
    "**Detailed Surrogate Loss for GRPO**\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)} \\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "- \\beta \\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $G$ is the number of sampled outputs per prompt  \n",
    "- $|o_i|$ is the token length of output $o_i$  \n",
    "- $r_{i,\\tt}(\\theta) = \\frac{\\pi_\\theta(o_{i,\\tt} | q, o_{i,<\\tt})}{\\pi_{\\theta_{\\mathrm{old}}}(o_{i,\\tt} | q, o_{i,<\\tt})}$ is the per-token importance sampling ratio  \n",
    "- $\\hat{A}_{i,\\tt}$ is the group-relative normalized advantage for token $\\tt$ in sample $i$  \n",
    "- $\\beta$ is the KL penalty coefficient controlling divergence from\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    # GRPO training for LLM\n",
    "    for prompt in training_prompts:\n",
    "        outputs = [llm.generate(prompt) for _ in range(group_size)]\n",
    "        advantages = compute_group_relative_advantages(outputs, prompt) # e.g., using human rank or scoring function\n",
    "\n",
    "        # Compute loss for all outputs (favor those with high relative advantage)\n",
    "        losses = []\n",
    "        for output, advantage in zip(outputs, advantages):\n",
    "            logprob = llm.logprob(output, prompt)\n",
    "            losses.append(-logprob * advantage)\n",
    "\n",
    "        loss = sum(losses) / group_size\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Key Points: \n",
    "    \n",
    "    Multiple candidates sampled per prompt; each gets an advantage score; updates increase likelihood of better completions in the group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAPO: An Improved GRPO; Case Study of Real-World\n",
    "\n",
    "\n",
    "\n",
    "The GRPO model was introduced by DeepSeek, which published the algorithm pseudo-code.\n",
    "\n",
    "However, as is often the case, the \"baseline\" model that is published\n",
    "is *not sufficient* to replicate the performance results quoted in the paper.\n",
    "\n",
    "There are substantial \"engineering details\" that are not disclosed.\n",
    "- Published DeepSeek results on GRPO\n",
    "    - perform and incremental 50% higher than baseline GRPO on some benchmarks\n",
    "- Details matter !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Responding to this, ByteDance introduced a new model:\n",
    "\n",
    "- *Decoupled\n",
    "Clip and Dynamic sAmpling Policy Optimization (DAPO)*\n",
    "\n",
    "in the paper:\n",
    "[DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/pdf/2503.14476).\n",
    "\n",
    "DAPO is\n",
    "\n",
    "- fully Open Source\n",
    "- that provides refinements to the baseline GRPO\n",
    "- sufficient to achieve similar results to the published DeepSeek paper\n",
    "\n",
    "This was the result of\n",
    "- experimentation\n",
    "- error analysis\n",
    "\n",
    "to diagnose the failures of baseline GRPO and devise remedies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DAPO introduces 4 key modifications on baseline GRPO\n",
    "- Changing the threshold for clipping\n",
    "- *Eliminating* non-informative samples \n",
    "- Careful redefinition of Token-level Gradient Loss\n",
    "- Penalizing overly long responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DAPO uses a Surrogate Loss that differs from that of GRPO\n",
    "- in *subtle* ways\n",
    "\n",
    "To highlight the differences in Surrogate Loss\n",
    "- we incrementally modify $J_{\\mathrm{GRPO}}(\\theta)$\n",
    "- to obtain $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "\n",
    "We defer the justification for the modifications until after the final $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "has been derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evolution of $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "\n",
    "Let's start with the Surrogate Loss for GRPO:\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)} \\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "- \\beta \\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\mathcal{D}$ is the training dataset\n",
    "    - consisting of examples consisting of question/answer pairs\n",
    "    $$(q, a)$$\n",
    "\n",
    "Recall that GRPO\n",
    "- creates a group of size $G$ answers to $q$\n",
    "- by sampling from the LLM with non-zero temperature\n",
    "\n",
    "We modify the  GRPO Surrogate Loss in increments to obtain the Surrogate Loss for DAPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remove KL constraint\n",
    "\n",
    "First, observe that, relative to GRPO\n",
    "- DAPO removes the KL-divergence constraint\n",
    "$$\n",
    "\\mathrm{D}_{\\mathrm{KL}}(\\pi_\\theta || \\pi_{\\mathrm{ref}})\n",
    "$$\n",
    "\n",
    "that limits how far\n",
    "- new policy $\\pi_\\theta$\n",
    "- can diverge from the *reference* policy $\\pi_\\text{ref}$.\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO^1}}(\\theta) = \\mathbb{E}_{(q,a) \\sim D, \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot|q)} \\left[\n",
    "\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "giving us the intermediate $J_{\\mathrm{DAPO^1}}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Changing the token-level average\n",
    "\n",
    "\n",
    "In GRPO\n",
    "- First averages token losses within each sample by its length $ |o_i| $,\n",
    "- Then averages sample losses uniformly over group $G$.\n",
    "\n",
    "DAPO\n",
    "- Sums all token losses in the group before normalizing by total tokens in all samples.\n",
    "\n",
    "giving us the intermediate \n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO^2}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{ o_i \\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot | q)}\n",
    "\\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|}\n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This is referred to as the *Token-level Loss* technique in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Asymmetric Clipping \n",
    "\n",
    "GRPO's clipping is symmetric with range\n",
    "$$\n",
    "[ 1 - \\epsilon, 1 + \\epsilon ]\n",
    "$$\n",
    "\n",
    "\n",
    "DAPO adds different clipping values on either side\n",
    "$$\n",
    "[ 1 - \\epsilon_{\\mathrm{low}}, 1 + \\epsilon_{\\mathrm{high}} ]\n",
    "$$\n",
    "\n",
    "giving us the intermediate \n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO^3}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{ o_i \\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot | q)}\n",
    "\\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|}\n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon_{\\mathrm{low}}, 1 + \\epsilon_{\\mathrm{high}} \\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This is referred to in the paper as the *Clip Higher* technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic sampling\n",
    "\n",
    "GRPO's expectation is over\n",
    "- **all** trajectories in the training dataset\n",
    "\n",
    "DAPO limits the Group composition\n",
    "- to ensure that the intra-group trajectory rewards **are not identical**\n",
    "\n",
    "\n",
    "giving us the final\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a) \\sim \\mathcal{D}, \\{ o_i \\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot | q)}\n",
    "\\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|}\n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}\\left(r_{i,\\tt}(\\theta), 1 - \\epsilon_{\\mathrm{low}}, 1 + \\epsilon_{\\mathrm{high}} \\right) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right],\n",
    "$$\n",
    "subject to:\n",
    "$$\n",
    "0 < \\big| \\{ o_i \\mid \\text{is_equivalent}(a, o_i) \\} \\big| < G\n",
    "$$\n",
    "\n",
    "where $a$ is the (single) response in the training dataset $\\mathcal{D}$\n",
    "- before sampling to generate $G$ sample responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRPO vs DAPO Comparison\n",
    "\n",
    "\n",
    "| Feature                    | GRPO                                           | DAPO                                              |\n",
    "|:----------------------------|:------------------------------------------------|:---------------------------------------------------|\n",
    "| Loss Aggregation           | $ \\frac{1}{G}\\sum_i \\frac{1}{|o_i|} \\sum_t $ | $ \\frac{1}{\\sum_i |o_i|} \\sum_i \\sum_t $        |                            |\n",
    "| Effect on Long Responses   | Weighed less per token (gradient dilution)     | Maintains per-token gradient magnitude            |\n",
    "| Clipping                  | Symmetric $(1-\\epsilon, 1+\\epsilon)$            | Asymmetric $(1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}})$ |\n",
    "| Impact                    | Slower learning on longer tokens                | Finer control, better learning on complex outputs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Aspect                | GRPO                                               | DAPO                                                        |\n",
    "|:----------------------|:---------------------------------------------------|:------------------------------------------------------------|\n",
    "| Value Model Dependency| Removes PPO value model, uses relative group rewards| Same, but more robust and stable optimization               |\n",
    "| Clipping Mechanism    | Basic clip bounds on importance ratios              | Asymmetric Clip-Higher for rare, valuable tokens            |\n",
    "| Sampling              | Multi-response batch, potentially redundant         | Dynamic Sampling ensures diverse, effective samples         |\n",
    "| Gradient Weighting    | Token-level, gradient diluted in long outputs       | Token-Level Gradient Loss corrects signal dilution          |\n",
    "| Reward for Length     | None or truncation-based                            | Overlong Reward Shaping with soft penalties                 |\n",
    "| Efficiency/Stability  | Improved over PPO, but unstable for some scenarios | Superior stability and efficiency; state-of-the-art results |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Feature             | PPO                                                      | GRPO                                                     | DAPO                                                           |\n",
    "|:--------------------|:---------------------------------------------------------|:---------------------------------------------------------|:---------------------------------------------------------------|\n",
    "| Advantage Source    | Value model estimate                                     | Group-relative, reward normalized in batch                | Same as GRPO                                                   |\n",
    "| Clipping            | Symmetric ($1-\\epsilon$, $1+\\epsilon$)                   | Symmetric ($1-\\epsilon$, $1+\\epsilon$)                    | Asymmetric ($1-\\epsilon_\\text{low}$, $1+\\epsilon_\\text{high}$) (Clip-Higher) |\n",
    "| Loss Aggregation    | Per-sample/token                                         | Per-sample, then averaged over tokens                     | Per-token, avoids dilution in long outputs                     |\n",
    "| Sampling            | Independent samples                                      | Groups of samples per prompt                              | Dynamic: only informative samples (not all-correct/incorrect)   |\n",
    "| Reward Shaping      | None by default                                          | None by default                                           | Overlong shaping (discourages excessive length)                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KL-constraint elimination: why ?\n",
    "\n",
    "For many RL objectives (e.g., induce long CoT reasoning in the response)\n",
    "- the *new distribution*\n",
    "- is necessarily *far* from the reference distribution\n",
    "\n",
    "So the KL constraint is too limiting for some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Speculation**\n",
    "\n",
    "DeepSeek initially tried to induce reasoning \n",
    "- using **only** RL\n",
    "- **no** SFT step before the RL\n",
    "\n",
    "As we have seen\n",
    "- SFT \"primes the pump\" to make RL more likely to succeed\n",
    "- by ensuring the initial RL model is able to produce\n",
    "    - correctly formatted responses\n",
    "    \n",
    "Perhaps the KL-constraint is a vestige of the RL-only attempt\n",
    "- since the long CoT desired model was far different from the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Token-level averaging change: why ?\n",
    "\n",
    "The GRPO loss aggregation\n",
    "$$\n",
    "\\frac{1}{G}\\sum_i \\frac{1}{|o_i|} \\sum_\\tt \\ldots\n",
    "$$\n",
    "\n",
    "creates a loss for sample $i$ in group $g$\n",
    "- that is the *average over the tokens* of response $o_i$\n",
    "\n",
    "before averaging the sample loss over the $G$ elements of the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This introduces a *bias*\n",
    "- the gradient update of an \"important\" token in a **long** response $o_i$\n",
    "    - important: high gradient\n",
    "- has less weight in the Gradient of $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "- than a token of *equal importance* in a **shorter** response $o_{i'}$\n",
    "\n",
    "even when the advantages $\\hat A_{i}$ and $\\hat A_{i'}$ are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward hacking\n",
    "\n",
    "This can lead to the undesirable phenomenon known as *Reward Hacking*.\n",
    "\n",
    "RL learns to produce *short **correct** responses* \n",
    "- with high trajectory (and thus, high per-token) reward\n",
    "- to amplify the contribution of important tokens\n",
    "\n",
    "and *long **incorrect** responses*\n",
    "- with low trajectory (and thus, low per-token) reward\n",
    "- to dilute the contribution of important tokens\n",
    "    - adding gibberish or repetitive content in order to increase length\n",
    "\n",
    "The result is that\n",
    "- the impact of important tokens\n",
    "- on updating the policy \n",
    "- is distorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DAPO eliminates this bias by changing the aggregation\n",
    "\n",
    " $ \\frac{1}{\\sum_i |o_i|} \\sum_i \\sum_\\tt \\ldots $\n",
    " \n",
    "so that\n",
    "- *all tokens in all responses in a group*\n",
    "- have the same contribution to the Gradient of $J_{\\mathrm{DAPO}}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward shaping: Overlong reward shaping length penalty\n",
    "\n",
    "DAPO also adds a *length penalty* to directly discourage excessively long responses\n",
    "\n",
    "$$\n",
    "\\tilde{R}_i = R_i - \\alpha \\cdot \\text{LengthPenalty}(o_i)\n",
    "$$ \n",
    "\n",
    "where $\\text{LengthPenalty}(o_i)$ measures the  undesirable properties of response $o_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The penalty used in the paper is called *Soft Overlong Punishment*\n",
    "- two length thresholds\n",
    "- length penalty is phased in between the thresholds\n",
    "\n",
    "In addition\n",
    "- long responses are truncated to a maximum length\n",
    "- the truncated elements of the response are masked in the loss calculation\n",
    "\n",
    "This is referred to as *Overlong Filtering* in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Asymmetric Clipping: why ?\n",
    "\n",
    "\n",
    "Recall the *probability ratio*\n",
    "\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\n",
    "$$ \n",
    "\n",
    "expresses how much \n",
    "- the *new probability* $\\pi_\\theta(a_t|s_t)$ of an action (given a state)\n",
    "- can differ  from\n",
    "- the *old probability* $\\pi_{\\theta_{\\text{old}}}(a_t|s_t)$\n",
    "- in *proportional terms*\n",
    "\n",
    "This ratio is *clipped* in GRPO and DAPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "In the GRPO loss\n",
    "- the clipping range is \n",
    "$$\n",
    "[ 1-\\epsilon, 1+\\epsilon ]\n",
    "$$\n",
    "\n",
    "Typically:\n",
    "$$\n",
    "\\epsilon = .20\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "In DAPO\n",
    "- the clipping range is adjusted to\n",
    "$$1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}}\n",
    "$$\n",
    "\n",
    "Typically\n",
    "- $\\epsilon_{\\mathrm{low}} = \\epsilon = .2$\n",
    "- $\\epsilon_{\\mathrm{high}} = .28$\n",
    "\n",
    "Thus, the probability in DAPO is allowed to increase\n",
    "- more (proportionally) than it is allowed to decrease\n",
    "\n",
    "The reason for doing so is a simple consequence of \n",
    "- translating proportional increase to absolute increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Given two actions/tokens  at step $\\tt$\n",
    "$$a_{t, \\text{low}}, a_{t, \\text{high}}$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{low}} |s_t) \\lt \n",
    "\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{high}} |s_t)\n",
    "$$\n",
    "\n",
    "the absolute increase in probability for $a_{t, \\text{low}}$ is less than for $a_{t, \\text{high}}$\n",
    "- since the initial probability $\\pi_{\\theta_{\\text{old}}}(a_{t, \\text{low}} |s_t)$ is lower\n",
    "- even if $\\epsilon_{\\mathrm{high}} = \\epsilon_{\\mathrm{}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When both actions/tokens are potentially high importance *exploration* (rather than exploitation) actions\n",
    "- the potential *reward* benefit for $a_{t, \\text{low}}$\n",
    "- is lower than for $a_{t, \\text{high}}$\n",
    "\n",
    "The RL may learn to favor the (potentially less important in terms of global goal) $a_{t, \\text{high}}$ over $a_{t, \\text{low}}$.\n",
    "\n",
    "This has the negative effect of \n",
    "- diminishing the reward benefit of some exploration actions\n",
    "- that *could* be high value in moving the policy in the optimal direction\n",
    "\n",
    "thus restricting profitable exploration.\n",
    "\n",
    "Raising the upper clipping bound has the effect of increasing the entropy of the policy\n",
    "- more exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dynamic sampling: why ? \n",
    "\n",
    "If all samples in a group have the same trajectory reward\n",
    "- the Advantage of each sample in the group\n",
    "- is mathematically equal to $0$\n",
    "\n",
    "Groups with samples having identical reward are typically\n",
    "- all samples are correct\n",
    "- all samples are incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because the advatange of each sample is $0$, these groups\n",
    "- do not contribute to the Gradient of $J_{\\mathrm{DAPO}}(\\theta)$\n",
    "\n",
    "When computing the Gradient of a batch of $N$ questions\n",
    "- each with $G$ sample responses\n",
    "\n",
    "the groups with $0$ advantage reduce the effective batch size.\n",
    "\n",
    "This results in such batches having *noisier* gradient updates than unaffected batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover the learning signal\n",
    "- is stronger\n",
    "- when there is a *contrast* between samples\n",
    "    - high reward vs. low reward\n",
    "- whether within a group or within a batch\n",
    "\n",
    "Dynamic sampling promotes contrastive groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the topic of *contrastive examples*\n",
    "- without the initial SFT of RFT\n",
    "- when the behavior to learn via RL is very different than the behavior of the base LLM\n",
    "- all examples and samples are likely to have the same low reward\n",
    "    - because of incorrect formatting or logic\n",
    "    \n",
    "So the initial SFT will hopefully create some high reward examples\n",
    "- by \"moving the distribution\" of RL training examples\n",
    "- closer to the ultimate RL goal\n",
    "- than the distribution of examples from the base (non-SFT tuned) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contribution of each technique to the improvement of DAPO vs GRPO\n",
    "\n",
    "| Technique              | Description                                                                                  | Commentary                                                                                  | Accuracy Improvement (AIME 2024 avg@32) |\n",
    "|:-----------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------------------------------|\n",
    "| Naive GRPO             | Baseline group relative policy optimization without enhancements                            | Starting point with relatively low accuracy                                                 | 30                                       |\n",
    "| Overlong Filtering     | Filters out truncated (overlong) samples from training loss                                | Reduces reward noise caused by forced truncation, stabilizes training                       | 36                                       |\n",
    "| Clip-Higher            | Decouples lower and upper clipping range to allow higher increase for low-probability tokens | Enhances policy entropy and exploration, avoids early collapse of exploration              | 38                                       |\n",
    "| Soft Overlong Punishment| Length-aware penalty on excessively long responses                                        | Prevents reward noise from overly penalizing valid but long reasoning chains                | 41                                       |\n",
    "| Token-Level Loss       | Aggregates loss over tokens normalized by total token count (not per sample average)       | Improves training stability and healthier growth in output length                           | 42                                       |\n",
    "| Dynamic Sampling       | Oversamples and filters batches to keep effective gradient signals by excluding zero-advantage samples | Significantly improves training stability and performance speed                            | 50                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pseudo code for DAPO\n",
    "\n",
    "**Detailed Surrogate Loss for DAPO**\n",
    "\n",
    "$$\n",
    "J_{\\mathrm{DAPO}}(\\theta) =\n",
    "\\mathbb{E}_{(q,a), \\{o_i\\} \\sim \\pi_{\\theta_{\\text{old}}}} \\left[\n",
    "\\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{\\tt=1}^{|o_i|} \n",
    "\\min{} \\left(\n",
    "r_{i,\\tt}(\\theta) \\hat{A}_{i,\\tt},\\;\n",
    "\\mathrm{clip}(r_{i,\\tt}(\\theta), 1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}}) \\hat{A}_{i,\\tt}\n",
    "\\right)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    # Given:\n",
    "    # batch_size = N\n",
    "    # num_samples = K\n",
    "    # sequence_lengths = [T_gi for each response i in group g]\n",
    "    # importance_scores = array of same shape as tokens, default = 1\n",
    "\n",
    "    for group_id in range(N):\n",
    "        for sample_id in range(K):\n",
    "            T = sequence_lengths[group_id][sample_id]\n",
    "            importance_sum = 0\n",
    "            # Compute sum of importance scores in sample\n",
    "            for t in range(T):\n",
    "                importance_sum += importance_scores[group_id][sample_id][t]\n",
    "            # Calculate alpha for each token\n",
    "            for t in range(T):\n",
    "                alpha = importance_scores[group_id][sample_id][t] / importance_sum\n",
    "                # Compute token-wise policy gradient component:\n",
    "                grad = alpha * w[group_id][sample_id] \\\n",
    "                       * clip(r[group_id][sample_id][t], 1-eps_low, 1+eps_high) \\\n",
    "                       * advantage[group_id][sample_id][t]\n",
    "                # Accumulate grad, update model params, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References for GRPO to DAPO**\n",
    "\n",
    "- [DAPO: An Open-Source LLM Reinforcement Learning System at Scale (arXiv)](https://arxiv.org/abs/2503.14476)\n",
    "- [Mathematics of DAPO, PPO, and GRPO (SSRN)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5205449)\n",
    "- [From GRPO to DAPO and GSPO: What, Why, and How (Hugging Face blog)](https://huggingface.co/blog/NormalUhr/grpo-to-dapo-and-gspo)\n",
    "- [The Evolution of GRPO: DAPO (Towards AI)](https://towardsai.net/p/l/the-evolution-of-grpo-dapo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Direct Preference Optimization (DPO)\n",
    "\n",
    "We now present a **Supervised Training** method for Preference data\n",
    "- **not** Reinforcement Learning\n",
    "- similar to Binary Classification\n",
    "    - compute probability that $y^+$ is preferred\n",
    "- no reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Direct Preference Optimization (DPO)** is a policy optimization method \n",
    "where supervision\n",
    "- comes from *Preference Data*\n",
    "- rather than rewards\n",
    "\n",
    "$$\n",
    "(x, y^+, y^-)\n",
    "$$\n",
    "\n",
    "The policy is trained to assign a higher likelihood to the preferred outcome $y^+$ than the \n",
    "non-preferred outcome $y^-$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relation to the Unified Gradient Formulation\n",
    "\n",
    "DPO also uses a surrogate loss to guide the derivation of the policy.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)} \\right)\n",
    "= -\\log \\sigma \\left( \\Delta \\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\pi_\\theta (y^+ | x )$\n",
    "- $\\pi_\\theta (y^- | x )$\n",
    "\n",
    "\n",
    "are the probabilities of the *trajectories* resulting in outputs $y^+$ and $y^-$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The ratio\n",
    "$$\n",
    "\\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)}\n",
    "$$\n",
    "\n",
    "is the relative probability of the preferred output, compared to the non-preferred output._\n",
    "\n",
    "- We take the log of the ratio\n",
    "    - resulting in log-probabilities, as in the Universal Formulation\n",
    "\n",
    "- The sigmoid $\\sigma$ converts the relative probability to the range $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can compute the gradient of $L_{\\mathrm{DPO}}(\\theta)$\n",
    "\n",
    "We first simplify $L_{\\mathrm{DPO}}(\\theta)$ by defining\n",
    "\n",
    "$$\n",
    "\\Delta = \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)}  = \\log \\pi_\\theta(y^+|x) - \\log \\pi_\\theta(y^-|x)\n",
    "$$\n",
    "\n",
    "Substituting into  $L_{\\mathrm{DPO}}(\\theta)$\n",
    "\n",
    "$$\n",
    "L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\Delta \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient of this simplified $L_{\\mathrm{DPO}}(\\theta)$ is\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\nabla_\\theta L_{\\mathrm{DPO}}(\\theta) & = & -(1 - \\sigma(\\Delta)) \\cdot \\nabla_\\theta \\Delta \\\\\n",
    "& = & -(1 - \\sigma(\\Delta)) \\cdot \n",
    "\\big( \n",
    "\\nabla_\\theta \\log \\pi_\\theta(y^+|x)\n",
    "-\n",
    "\\nabla_\\theta \\log \\pi_\\theta(y^-|x)\n",
    "\\big)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This follows from basic rules of calculus\n",
    "- $\\nabla_\\theta \\log  \\sigma(\\Delta ) = \\frac{1}{\\sigma(\\Delta ) } \\nabla_\\theta \\sigma(\\Delta ) $\n",
    "- $\\nabla_\\theta \\sigma(\\Delta ) = \\sigma(\\Delta) \\big( 1 - \\sigma(\\Delta) \\big) * \\nabla_\\Theta \\Delta$\n",
    "    - since $\\sigma(\\Delta) = \\frac{1}{1 + e^{-\\Delta}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The term \n",
    "$$\n",
    "(1 - \\sigma(\\Delta))\n",
    "$$\n",
    "\n",
    "is interpreted as the *Advantage* of $y^+$ over $y^-$.\n",
    "\n",
    "This advantage is small\n",
    "- when $\\sigma(\\Delta) \\approx 1$\n",
    "    - i.e., the model is confident: assigning high probability to the preferred output\n",
    "\n",
    "Conversely, it is large when the model is uncertain.\n",
    "\n",
    "So the advantage term adjusts the Gradient update step size depending on how far the\n",
    "probability of the preferred output is from 100%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient can be interpreted as adjusting the policy\n",
    "- so as to increase the (log) likelihood\n",
    "- adjusted by the advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Supervised Training vs Reinforcement Learning**\n",
    "\n",
    "You will notice that, in DPO\n",
    "- the classic elements of RL are missing\n",
    "    - supervision via Loss function, not rewards\n",
    "    - direct preferences rather than implicit/explicit rewards\n",
    "        - reward sparsity not an issue; just trajectory preference\n",
    "    - no reference to states/actions/rewards\n",
    "\n",
    "Fine-tuning a model with DPO is Supervised Fine Tuning rather than Reward Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**$\n",
    "L_{\\mathrm{DPO}}(\\theta) = -\\log \\sigma \\left( \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)} \\right)\n",
    "= -\\log \\sigma \\left( \\Delta \\right)\n",
    "$ interpretation**\n",
    "\n",
    "We can interpret $L_{\\mathrm{DPO}}(\\theta)$ \n",
    "- in an identical manner \n",
    "- to how we interpreted \n",
    "$$\\loss^\\ip_\\mathrm{reward}(\\phi, x, y^+, y^-)$$\n",
    "\n",
    "for the Reward model of PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is:\n",
    "\n",
    "$L_{\\mathrm{DPO}}(\\theta)$ is equivalent to the Binary Cross Entropy loss for the problem \n",
    "predicting the probability\n",
    "$$\n",
    "\\prc{y^+ \\text{ preferred to } y^-}{x}\n",
    "$$\n",
    "where the score $z$ is\n",
    "$$\n",
    "z =  \\log \\frac{\\pi_\\theta(y^+|x)}{\\pi_\\theta(y^-|x)} = \\log \\pi_\\theta(y^+|x) - \\log \\pi_\\theta(y^-|x)\n",
    "$$\n",
    "\n",
    "From the perspective of $L_{\\mathrm{DPO}}(\\theta)$\n",
    "- we are performing Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DPO vs PPO\n",
    "\n",
    "PPO for Preferences involves one sub-model\n",
    "- Reward Model\n",
    "\n",
    "**in addition to** the sub-models inherent in PPO\n",
    "- policy\n",
    "- reference\n",
    "- Value/Critic: for advantage computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All of these models are roughly the same size\n",
    "- so PPO is more expensive than DPO\n",
    "    - training data for each PPO sub-model\n",
    "    - similar number of parameters for each sub-model\n",
    "    \n",
    "DPO, by contrast, is a single model\n",
    "- more accessible in terms of reduced compute/memory footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Approach         | Data Used                  | Training Steps                            | Model Copies Required          | Key Challenge                          |\n",
    "|:------------------|:----------------------------|:-----------------------------------------|:-------------------------------|:--------------------------------------|\n",
    "| PPO + Reward Model| Preference  Scalar rewards | Train reward model, then PPO optimization | Policy, reference, value, reward | Reward modeling complexity, instability |\n",
    "| DPO              | Preference pairs directly  | Single-stage policy gradient optimization | Only policy                   | Requires careful pairing, but simpler overall |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Pseudo code for DPO\n",
    " \n",
    "    # DPO training for LLM\n",
    "    for prompt in training_prompts:\n",
    "        outputs = [llm.generate(prompt) for _ in range(2)]\n",
    "        \n",
    "        # outputs: [output_0, output_1]\n",
    "        # preference: 0 if output_0 is preferred, 1 if output_1 is preferred\n",
    "        preferred_idx = compare_outputs(outputs) # Human or synthetic comparison\n",
    "\n",
    "        logit_pref = llm.score(outputs[preferred_idx], prompt)\n",
    "        logit_nonpref = llm.score(outputs[1 - preferred_idx], prompt)\n",
    "\n",
    "        # DPO loss: maximize difference so preferred > non-preferred\n",
    "        loss = -logsigmoid(logit_pref - logit_nonpref)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison of methods for Preference Data\n",
    "\n",
    "| Aspect              | PPO (Proximal Policy Optimization)                  | DPO (Direct Preference Optimization)                      | GRPO (Group Relative Policy Optimization)                    |\n",
    "|:---------------------|:------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------|\n",
    "| **Stability**       | Moderate stability, uses clipped objective to limit policy updates and prevent divergence. Can still be sensitive to reward noise and hyperparameters. | High stability due to supervised-learning style objective on preference pairs. Does not rely on policy gradient RL steps. | Higher stability than PPO due to normalized group rewards reducing gradient noise; does not require a value function critic which reduces instability. |\n",
    "| **Variance**        | High variance in gradient estimates caused by sparse rewards and stochastic policy sampling. Requires variance reduction techniques (e.g., baseline/critic). | Low variance because gradients come from direct supervised preference comparisons without sampling or policy gradients. | Moderate variancevariance is reduced by reward normalization within groups but still involves sampling multiple outputs, so more variance than DPO but less than PPO. |\n",
    "| **Sample Efficiency** | Moderate to lowneeds many environment interactions/samples due to sparse reward signal and on-policy updates. Sampling multiple sequences per prompt increases cost. | Very hightrains directly on labeled preference pairs with no complex sampling or reward modeling. | Higher than PPOrequires multiple samples per prompt for group comparison but gains efficiency from relative advantage normalization and critic-free updates. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
