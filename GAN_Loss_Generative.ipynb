{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb\n",
    "%run beautify_plots.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does the GAN make $\\pdata \\approx \\pmodel$ ?\n",
    "\n",
    "The Generator Loss function we constructed is a proxy to achieve the goal\n",
    "\n",
    "$$\\pmodel \\approx \\pdata$$\n",
    "\n",
    "That is: the distribution of samples produced by the Generator is (approximately) the same as the \"true\" distribution\n",
    "- we note that we don't know the \"true\" $\\pdata$\n",
    "    - we only have available a sample and those the training set defines an *empirical* distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are several ways to quantify\n",
    "\n",
    "$$\\pmodel \\approx \\pdata$$\n",
    "\n",
    "One choice would be the minimization of KL Divergence\n",
    "- $\\KL( \\pdata || \\pmodel)$\n",
    "\n",
    "An alternative, still using KL Divergence\n",
    "- $\\KL( \\pmodel || \\pdata )$\n",
    "\n",
    "Which is a better choice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to answer the question, we begin with a few preliminaries\n",
    "- Definition of KL Divergence\n",
    "- Proving \n",
    "    - minimizing KL Divergence increases log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definition of KL Divergence\n",
    "\n",
    "As a reminder of the definition of KL Divergence\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\text{KL}(p || q ) \n",
    "& = & -\\sum\\limits_{x}p(x) \\log q(x) + \\sum\\limits_{x}p(x) \\log p(x)  \\\\\n",
    "& = & \\sum\\limits_{x} {  p(x) * \\left( \\log p(x) - \\log q(x) \\right) } \\\\\n",
    "& = & \\E_{\\x \\sim p } { \\left( \\log p(x) - \\log q(x) \\right) } \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You can see that it is\n",
    "- the point-wise difference between the (log) probability of $\\x$ in distributions $p$ and $q$\n",
    "- averaged over the distribution of $\\x \\sim p$\n",
    "\n",
    "and thus is a point-wise measure of the dis-similarity of the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We note that the KL Divergence is *not symmetric*\n",
    "$$\n",
    "\\KL( \\pdata || \\pmodel) \\ne \\KL( \\pmodel || \\pdata )\n",
    "$$\n",
    " so the two choices are different.\n",
    " - both are expectations\n",
    " - but over *different* distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KL Divergence leads to Maximum Likelihood Estimation\n",
    "\n",
    "We now show that using $\\KL( \\pdata || \\pmodel )$ as a loss function\n",
    "- results in a estimation of the model distribution  $\\pmodel$\n",
    "- that is the Maximum Likelihood estimator of the training examples (represented by $\\pdata$)\n",
    "\n",
    "That is\n",
    "- $\\pmodel$ is the best explanation of the training dataset $\\pdata$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Choosing $\\pmodel$ to Minimize gives\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\KL( \\pdata || \\pmodel ) & = & \\int_\\x  { \\pdata(\\x) \\, \\left( \\log\\frac{\\pdata(\\x)}{\\pmodel\\x)} \\right) }{d\\x} & \\text{Definition of KL Divergence} \\\\\n",
    "& = & \\E_{\\x \\in \\pdata} \\log(\\pdata(\\x)) - \\log(\\pmodel(\\x)) & \\text{Definiton of log of ratio as difference in logs} \\\\\n",
    "\\text{minimizing KL} \\\\\n",
    "& \\approx & \\E_{\\x \\in \\pdata} - \\log(\\pmodel(\\x))  & \\text{Since } \\log(\\pdata(\\x)) \\text{ is only a constant in the term being minimized} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So minimizing $\\KL$ is equivalent to \n",
    "- minimizing the Negative Log Likelihood\n",
    "- in other words: *maximizing* the Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing the KL Divergence\n",
    "\n",
    "The first choice \n",
    "$$\n",
    "\\KL( \\pdata || \\pmodel) = \\E_{\\x \\sim \\pdata } { \\left( \\log \\pdata (x) - \\log \n",
    "\\pmodel(x) \\right) }\n",
    "$$\n",
    "\n",
    "maximizes $\\log(\\pmodel(\\x))$ for $\\x \\in \\pdata$\n",
    "- $\\pmodel$ assigns high probability to Real examples\n",
    "- model creates Real examples with high probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By way of analogy with measures for Classification\n",
    "- the expectation over $\\pdata$ emphasizes Recall over Precision\n",
    "\n",
    "We can achieve high Recall\n",
    "- by reducing chance of False Negatives (FN)\n",
    "- even if it increases chance of False Positives (FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the GAN context this means\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\text{reducing FN}   \\leadsto \\pmodel \\text{ assigns high probability to each training example in } \\pdata & \\text{fidelity to training data}\\\\\n",
    "\\text{increasing FP} \\leadsto \\pmodel \\text{ assigns high probability to } \\x \\notin \\pdata \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second choice $\\KL( \\pmodel || \\pdata )$\n",
    "\n",
    "$$\n",
    "\\KL( \\pmodel || \\pdata) = \\E_{\\x \\sim \\pmodel } { \\left( \\log \\pmodel (x) - \\log \n",
    "\\pdata (x) \\right) } \n",
    "$$\n",
    "\n",
    "maximizes $\\pdata(x)$ for $\\x \\in \\pmodel$\n",
    "- emphasizes that synthetic examples are \"realistic\"\n",
    "    - highly probable, as defined by the empirical distribution (training data) $\\pdata$\n",
    "    \n",
    "This (\"realistic examples\") might be the more desirable property than \"high fidelity\" to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Continuing with our Recall versus Precision analogy, this measure\n",
    "- increases Precision by reducing False Positives\n",
    "    - examples generated by $\\pmodel$ are likely according to $\\pdata$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So it seems as if the second choice  $\\KL( \\pmodel || \\pdata )$ may be more desirable.\n",
    "\n",
    "**But** we don't know the true $\\pdata$ !\n",
    "- we only have an empirical sample: the training dataset\n",
    "- so, in practical terms: we can't maximize it\n",
    "\n",
    "Thus, practical considerations lead us to the first choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jensen-Shannon Divergence\n",
    "\n",
    "We have observed that the KL divergence is *not* symmetric\n",
    "$$\n",
    "\\KL( P || Q ) \\ne \\KL( Q || P )\n",
    "$$\n",
    "because the expectations are taken over different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An alternative measure of similarity of two distributions is the Jensen-Shannon Divergence (JSD)\n",
    "\n",
    "  $$\n",
    "    \\begin{array} \\\\\n",
    "    \\text{JSD}( P || Q ) & = & \\text{JSD}( Q || P )\\\\\n",
    "    & = & \\frac{1}{2} \\; \\text{KL} \\left( P \\, ||\\, \\frac{P+Q}{2} \\right) + \\\\\n",
    "    && \\frac{1}{2} \\; \\text{KL} \\left( Q \\, || \\, \\frac{P+Q}{2} \\right)\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "This measure is\n",
    "- symmetric\n",
    "- is a kind of mixture of $\\KL(P || Q)$ and $\\KL(Q || P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Huszar](https://arxiv.org/pdf/1511.05101.pdf) has a Generalized JSD which interpolates between the two terms\n",
    "$$\n",
    "    \\begin{array} \\\\\n",
    "    \\text{JSD}_\\pi( P || Q ) & = & \\text{JSD}( Q || P )\\\\\n",
    "    & = & \\pi \\; \\text{KL} \\left( \\,  P \\, ||\\, \\pi P + (1-\\pi) Q \\, \\right) + \\\\\n",
    "    && (1-\\pi) \\; \\text{KL} \\left( \\, Q \\, || \\, \\pi P + (1-\\pi) Q \\, \\right)\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "The Generalized JSD\n",
    "- **Not** symmetric although\n",
    "$$\n",
    "\\text{JSD}_\\pi( P || Q ) = \\text{JSD}_{1-\\pi}( Q || P )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Huszar shows that, for small values of $\\pi$\n",
    "$$\n",
    "\\frac{\n",
    "    \\text{JSD}_\\pi( P || Q )\n",
    "  }{\\pi}  \n",
    "    \\approx \\text{KL} \\left( \\,  P \\, ||\\, Q \\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\n",
    "    \\text{JSD}_{1-\\pi}( P || Q )\n",
    "  }{1-\\pi}  \n",
    "    \\approx \\text{KL} \\left( \\,  Q \\, ||\\, P \\right)\n",
    "$$\n",
    "\n",
    "In the first case\n",
    "- $\\text{JSD}_\\pi( P || Q )$ is proportional to Maximum Likelihood\n",
    "\n",
    "In the second case\n",
    "- $\\text{JSD}_{1-\\pi}( P || Q )$ is proportional to $\\text{KL} \\left( \\,  Q \\, ||\\, P \\right)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In implementing Generalized JSD\n",
    "- The Discriminator is trained (as usual) on a mix of real and fake examples\n",
    "    - But *not* in equal numbers\n",
    "    - $\\pi$ is fraction of  samples  from $Q$\n",
    "    - $(1-\\pi)$ is fraction of samples from $P$\n",
    "    - $\\pi \\lt \\frac{1}{2}$: real samples over represented\n",
    "    - $\\pi \\gt \\frac{1}{2}$: biased toward $Q$\n",
    "- Explains why we often see training with Generator updated twice for each update of Discriminator ?\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Training and the Jensen-Shannon Divergence\n",
    "\n",
    "The Discriminator Loss $\\loss_D$\n",
    "- summed over all examples \n",
    "    - (ignoring the $\\frac{1}{2}$ from the previous presentation where we assumed equal number of Real and Fake)\n",
    "    \n",
    "is\n",
    "$$\n",
    "\\begin{array} \\\\ \n",
    "\\loss_D \n",
    "& = &  - \\left(  \\E_{\\x^\\ip \\in \\pdata } { \\log D(\\x^\\ip) }  + \\E_{\\x^\\ip  \\in \\pmodel} { \\log \\left( 1 - D(\\x^\\ip)  \\right) } \\right) & D(G(\\z)) = \\x^\\ip \\text{ for fake examples}\\\\\n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "We also showed that the optimal Discriminator results in \n",
    "$$\n",
    "D^*(\\x) =  \\frac{\\pdata (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plugging $D^*(\\x)$ into $\\loss_D$ (Goodfellow Equation ):\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\ \n",
    "\\loss_D \n",
    "& = &  - \\left(  \\E_{\\x^\\ip \\in \\pdata } { \\log  \\frac{\\pdata (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)} }  + \\E_{\\x^\\ip  \\in \\pmodel} { \\log  \\frac{\\pmodel (\\x)}{ \\pmodel(\\x) +\\pdata(\\x)} } \\right) \\\\\n",
    "& = & \n",
    "- \\left(  \\KL( \\pdata || \\pmodel(\\x) +\\pdata(\\x)) + \\KL(\\pmodel ||  \\pmodel(\\x) +\\pdata(\\x) \\right) & \\text{Def. of } \\KL \\\\\n",
    "& = & - \\left( \\log 4 + \\KL( \\pdata || \\frac{\\pmodel(\\x) +\\pdata(\\x)}{2}) + \\KL(\\pmodel || \\frac{ \\pmodel(\\x) +\\pdata(\\x)}{2} \\right) & \\text{dividing second arg. of each KL term by 2}  \\\\\n",
    "& & & \\text{translates into } - \\log 2 \\text{ in expansion of each KL term} \\\\\n",
    "& & & \\text{into log form.} \\\\\n",
    "& & & \\text{The } \\log 4 \\text{ offsets this}  \\\\\n",
    "& = & - \\left( \\log 4  + 2 * \\text{JSD} (  \\pdata || \\pmodel ) \\right) & \\text{Def. of JSD}\\\\\n",
    " & & & \\text{this is Equation 6 of Goodfellow}\\\\\n",
    "\\end{array}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above equations shows that\n",
    "- minimizing KL Divergence (second line above)\n",
    "- under the assumption that the Discriminator can train to be the **optimal** adversary\n",
    "\n",
    "results in $\\loss_D$ becoming equivalent to Jensen-Shannon Distance (last line above)\n",
    "\n",
    "So solving the minimax optimally\n",
    "minimizes\n",
    "the JSD divergence between $\\pdata$ and $\\pmodel$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
