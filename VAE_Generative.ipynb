{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "\\newcommand{\\advseq}{\\mathbb{A}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "The \"Vanilla\" Autoeconder that we studied\n",
    "- the Encoder maps example $\\x^\\ip$\n",
    "- to a single latent representations $\\z^\\ip$\n",
    "- such that the Decoder tries to \"invert\" $\\z^\\ip$ to recover an approximation of $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The hope was that a slight perturbation in $\\z^\\ip$ \n",
    "- would invert into an $\\x_\\text{synthetic}^\\ip$\n",
    "- that was a member of the valid class of examples\n",
    "\n",
    "thus facilitating the use of the Decoder as a source of synthetic examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, this is what we get\n",
    "- if we invert latent represenations\n",
    "- that are near neighbors of $\\z^\\ip$\n",
    "\n",
    "As you see\n",
    "- even small changes to $\\z^\\ip$ result in corrupted (i.e., not members of the 10 classes of clothing items) examples\n",
    "\n",
    "<img src=\"images/autoencoder_perturb_single_img.png\" width=90%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This motivates the *Variational Autoencoder (VAE)* \n",
    "- an Encoder that maps example $\\x^\\ip$ to a *distribution* of latents\n",
    "- such that samples from the distribution\n",
    "- can be inverted by the Decoder\n",
    "- into reasonable approximations of $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VAE: An engineering perspective \n",
    "\n",
    "The Encoder part of a VAE, given input $\\x^\\ip$\n",
    "- Produces the parameters (e.g., $\\mu^\\ip, \\sigma^\\ip$) of a distributional form\n",
    "- Draws a sample from the distribution as its output $\\z^\\ip$\n",
    "\n",
    "Thus, the latent representation $\\z$ of a given $\\x$ is a probability distribution $\\qrs{\\z \\mid \\x}{\\Phi}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "$\\mu^\\ip$ and $\\sigma^\\ip$ are \n",
    "- vectors\n",
    "- computed values (and hence, functions of $\\x^\\ip$) and **not** parameters\n",
    "- so training learns a *function* from $\\x^\\ip$ to $\\mu^\\ip$ and $\\sigma^\\ip$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although the Encoder \n",
    "- creates a distribution of latents $\\qrs{\\z \\mid \\x}{\\Phi}$\n",
    "\n",
    "it will be necessary to constrain this distribution\n",
    "- to be an approximation of the \"true\" distribution $\\qr{\\z \\mid \\x}$ of latents\n",
    "\n",
    "Thus, when training the VAE, the Loss Function will have two terms\n",
    "- the Reconstruction Loss\n",
    "    - enforcing that the reconstruction  $\\tilde{x}^\\ip$ is close to the input $\\x^\\ip$\n",
    "- the Divergence Constraint\n",
    "    - ensuring that that constructed distribution $\\qrs{\\z | \\x}{\\Phi}$ is close to true distribution $\\qr{\\z | \\x}$\n",
    "$$\n",
    "\\KL( \\qrs{\\z | \\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is not just straightforward engineering.\n",
    "\n",
    "In fact: the architecture of the VAE was obtained from the math rather than vice-versa !\n",
    "\n",
    "We provide a brief overview of the mathematics.\n",
    "\n",
    "The interested reader is referred to a highly recommended [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "for a detailed presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Details\n",
    "\n",
    "**Notation summary**\n",
    "\n",
    "term &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| dimension &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | meaning \n",
    ":---|:---|:---\n",
    "$\\x$ | $n$ | Random variable for Input\n",
    "$\\tilde\\x$ | $n$ | Output: reconstructed $\\x$\n",
    "$\\z$ | $n' << n$ | Random variable for Latent representation\n",
    "$E$  | $\\mathbb{R}^n \\rightarrow \\mathbb{R}^{n'}$ | Encoder\n",
    "| | $E(\\x) = \\z $\n",
    "$D$  | $\\mathbb{R}^{n'} \\rightarrow \\mathbb{R}^n$ | Decoder\n",
    "| | $\\tilde\\x = D(\\z) $\n",
    "| | $\\tilde\\x = D( E(\\x) )$\n",
    "| | $\\tilde\\x \\approx \\x$\n",
    "$\\pr{\\x}$ | prob. distribution | *prior* distribution of Inputs\n",
    "          | | intractable.  Only have empirical.\n",
    "$\\qr{\\z}$ | prob. distribution | *prior* distribution of Latents\n",
    "$\\qr{\\z \\mid \\x}$ | prob. distribution| *posterior* marginal distribution of Latents given Input\n",
    "                  | | intractable\n",
    "$\\qrs{\\z \\mid \\x}{\\Phi}$ | Neural Network| NN to approximate $\\qr{\\z \\mid \\x}$ \n",
    "                         | | Encoder\n",
    "$\\prs{\\x \\mid \\z}{\\Theta}$ | Neural Network| NN to approximate $\\pr{\\x \\mid \\z}$ \n",
    "                  | | Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# VAE: Mathematical perspective\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The VAE has a very interesting <b>two part</b> Loss Function</li>\n",
    "        <ul>\n",
    "            <li>Reconstruction Loss, as in the Vanilla AE</li>\n",
    "            <li>Divergence Loss\n",
    "        </ul>\n",
    "        <li>The Reconstruction Loss is not sufficient</li>\n",
    "        <ul>\n",
    "            <li>Issues of intractability arise</li>\n",
    "            <li>The Divergence Loss skirts intractability</li>\n",
    "            <ul>\n",
    "                <li>By constraining the Encoder to produce a tractable distribution</li>\n",
    "            </ul>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributions of inputs, latents, outputs\n",
    "\n",
    "Let's pretend that we don't already know the architecture of a VAE\n",
    "- that the latent $\\z^\\ip$ is generated as a probability function of $\\mu^\\ip$ and $\\sigma^\\ip$ given input $\\x^\\ip$.\n",
    "\n",
    "Instead let\n",
    "- $\\x \\sim \\pr{\\x}$ denote\n",
    "    - an example $\\x$ drawn from the distribution of examples $\\pr{\\x}$\n",
    "- $\\z \\sim    \\qr{\\z}$ denote\n",
    "    - a latent drawn from the true distribution of latents $\\qr{\\z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We marginalize these distributions to \n",
    "- show the dependence between example $\\x^\\ip$ and its latent representations\n",
    "\n",
    "Let\n",
    "- $\\qr{\\z | \\x}$ denote the *conditional*  distribution of Latent, given an Input\n",
    "    - This is the distribution that is produced by the Encoder\n",
    "- $\\pr{\\x | \\z}$: the *conditional* distribution of Input, given a Latent\n",
    "    - This is the distribution that is produced by the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can go from the marginal back to the joint distribution by integration\n",
    "- $\\pr{\\x | \\z}$\n",
    "- over all $\\z \\in \\qr{\\z}$\n",
    "\n",
    "$$\n",
    "\\pr{\\x} = \\int_{\\z \\in \\qr{\\z}}{ \\pr{\\x | \\z} \\; \\qr{\\z} }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Operationally, this would result in a Decoder architecture\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 1</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B_0.png\" width=80%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some obvious concerns about the integral\n",
    "- It may be very expensive to draw many samples of $\\z$ from $\\qr{\\z}$ for each training example $\\x^\\ip$\n",
    "- There are many random choices of $\\z$ from $\\qr{\\z}$ which can't reconstruct $\\x^\\ip$\n",
    "    - i.e., $\\prs{\\x^\\ip | \\z'}{\\Theta} = 0$ for many $\\z'$\n",
    "\n",
    "\n",
    "We can improve our sampling by considering only those choices of $\\z$ that could generate $\\x$\n",
    "and re-write the objective as\n",
    "\n",
    "$$\n",
    "\\pr{\\x} = \\int_{ \\z \\in \\qr{\\z | \\x} } { \\pr{\\x | \\z} \\; \\qr{\\z} }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem: Some distributions aren't known !\n",
    "\n",
    "But there's a problem !\n",
    "- The distribution $\\pr{\\x}$ of inputs $\\x$ is *intractable*\n",
    "- We only have *empirical samples* from the distribution\n",
    "    - i.e., the training examples\n",
    "    - we don't know the functional form of the parent distribution\n",
    "    \n",
    "For instance: what is the distribution of images of Dogs ?\n",
    "\n",
    "Similarly\n",
    "- $\\qr{\\z | \\x}$ is intractable\n",
    "    - Recall: this is the intended output of the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operationalizing the mathematically-derived architecture\n",
    "\n",
    "We will side-step the intractability issues by defining Neural Networks to learn\n",
    "*approximations* \n",
    "- $\\qrs{\\z | \\x}{\\Phi}$\n",
    "    - a Neural Network (the Encoder)\n",
    "    - with parameters $\\Phi$\n",
    "    - that will use training examples\n",
    "- to learn an approximation of true $\\qr{\\z | \\x}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly\n",
    "- $\\prs{\\x | \\z}{\\Theta}$\n",
    "    - a Neural Network (the Decoder)\n",
    "    - with parameters $\\Theta$\n",
    "    - that will use training examples\n",
    "- to learn an approximation of true $\\pr{\\x | \\z}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting architecture:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>VAE derivation: 2</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE_derivation_B.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constraining the Encoder\n",
    "\n",
    "In order to ensure that the Encoder's\n",
    "- approximate distribution $\\qrs{\\z | \\x}{\\Phi}$\n",
    "- is close to true distribution $\\qr{\\z | \\x}$\n",
    "\n",
    "we add the Divergence Constraint to the Loss Function\n",
    "\n",
    "$$\n",
    "\\KL( \\qrs{\\z | \\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "The KL function is *not symmetric*\n",
    "$$\n",
    "\\KL( \\qrs{\\z | \\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) \\ne \\KL(  \\qr{\\z | \\x} ||\\; \\qrs{\\z | \\x}{\\Phi} )\n",
    "$$\n",
    "\n",
    "So the constraint, as written\n",
    "- says that each approximation \n",
    "- is close to the true value\n",
    "\n",
    "It *does not* say\n",
    "- that each true value\n",
    "- has a close approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Loss Function for training\n",
    "\n",
    "We can train the Encoder/Decoder pair with the objective that the\n",
    "reconstructed $\\tilde{\\x}^\\ip$ approximates the true $\\x^\\ip$ from the training set, across all examples $i$.\n",
    "\n",
    "One way of stating this is as a Maximum Likelihood:\n",
    "- Solve for the weights $\\Phi, \\Theta$\n",
    "- That maximize the (log) Likelihood of the set of reconstructions $\\tilde{\\X}$ reproducing the training set $\\X$\n",
    "\n",
    "Since our practice is to minimize Loss (rather than maximize an objective function)\n",
    "we write our loss as (negative of log) likelihood\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss & = & - \\log( \\pr{\\X} )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Minimizing $\\loss$ is equivalent to maximizing likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adding the KL divergence constraint to our Likelihood objective gives the loss function\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss  & = & - \\log(\\prs{\\x}{\\Theta}) + \\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) \\\\\n",
    "& = & \\loss_R + \\loss_D\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "which now has two objectives\n",
    "- Reconstruction loss $\\loss_R$: maximize the likelihood (by minimizing the negative likelihood)\n",
    "- Divergence constraint $\\loss_D$: $\\qrs{\\z|\\x}{\\Phi}$ must be close to $\\qr{\\z | \\x}$\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\loss_R & = & - \\log( p_\\theta(\\x ) ) \\\\\n",
    "\\loss_D & = & \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z | \\x} \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem: $\\loss$ contains an intractable term\n",
    "\n",
    "Looking closely as the Divergence Constraint\n",
    "$$\n",
    "\\loss_D  =  \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z | \\x} \\right) \n",
    "$$\n",
    "\n",
    "we note the presence of the term\n",
    "$$\n",
    "\\qr{\\z | \\x}\n",
    "$$\n",
    "\n",
    "which we identified as intractable.\n",
    "\n",
    "Hence\n",
    "- the Divergence Constraint\n",
    "- as written\n",
    "- cannot be operationalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will show (in the next section: lots of algebra !) that the loss can be re-written as\n",
    "$$\n",
    "\\loss = - \\E_{\\z \\sim \\qrs{\\z|\\x}{\\Phi}}\\left( \\log( \\prs{\\x|\\z}{\\Theta} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi}  \\; ||\\;  \\qr{\\z} )\n",
    "$$\n",
    "\n",
    "This is *almost* identical to our original express for $\\loss$ except\n",
    "- the intractable KL term\n",
    "$$\n",
    "\\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z | \\x} \\right)\n",
    "$$\n",
    "\n",
    "becomes \n",
    "$$\n",
    " \\KL \\left(  \\qrs{\\z|\\x}{\\Phi} \\; || \\; \\qr{\\z} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is still contains intractable\n",
    "$$\n",
    "\\qr{\\z}\n",
    "$$\n",
    "\n",
    "We can make this tractable\n",
    "- by making *an assumption* as to the functional form of $\\qr{\\z}$\n",
    "    - e.g., Normal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "it is easier to make an assumption about \n",
    "- *unconditional* distribution $\\qr{\\z}$\n",
    "\n",
    "rather than\n",
    "- *conditional* distribution $\\qr{\\z | \\x}$\n",
    "\n",
    "since we don't have to know the joint relationship between example $\\x$ and its latent $\\z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choosing $\\qr{\\z}$\n",
    "\n",
    "So what distribution should we use for the prior $\\qr{\\z}$ ?\n",
    "- It should be differentiable, since we use Gradient Descent for optimization\n",
    "- It should be tractable with a closed form (such as a normal)\n",
    "\n",
    "A *convenient* (but **not necessary**) choice for $\\qr{\\z}$ is normal\n",
    "- If we choose $\\qr{\\z}$ as normal, we can require $q_\\phi( \\z | \\x )$ to be normal too\n",
    "- The KL divergence between two normals is an easy to compute function of their means and standard deviations.\n",
    "    - the \"easy to compute\" part is the \"convenience\"\n",
    "    - See [VAE tutorial](https://arxiv.org/pdf/1606.05908.pdf) Section 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Re-parameterization trick\n",
    "\n",
    "There is still one impediment to training.\n",
    "\n",
    "It involves the random choice of $\\z \\sim \\qrs{\\z|\\x}{\\Phi}$ in\n",
    "\n",
    "$$\n",
    "\\loss_R = \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\prs{\\x | \\z}{\\Theta} ) \\right)\n",
    "$$\n",
    "\n",
    "This is not a problem in the forward pass.\n",
    "\n",
    "But in the backward pass we need to compute\n",
    "$$\n",
    "\\frac{\\loss_R}{\\partial \\Theta}\n",
    "$$\n",
    "\n",
    "How do we back propagte through a random choice ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"reparameterization trick\" redefines the random choice $\\z$ as\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll] \\\\\n",
    "\\mathbf{z}  & = & \\mathbf{\\mu}_\\theta(\\x) + \\mathbf{\\sigma}_\\theta(\\x) * \\mathbf{\\epsilon} \\\\\n",
    "\\mathbf{\\epsilon} & \\sim & N(0,1) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is\n",
    "- Once we have defined $\\qr{\\z}$ to be a Normal distribution\n",
    "- We can re-write an element of the distribution\n",
    "    - as the distribution's mean plus a random standardized Normal $\\epsilon$ times the distribution's standard deviation\n",
    "    \n",
    "In this formulation, the random variable $\\epsilon$ appears in a product term\n",
    "- We can differentiate the product with respect to $\\Theta$\n",
    "- $\\epsilon$ can be treated as a constant in $\\frac{\\partial \\epsilon}{\\partial \\Theta}$\n",
    "\n",
    "The Encoder design is now to produce\n",
    "(trainable parameters) $\\mu_\\Theta, \\sigma_\\Theta$\n",
    "- And $\\z$ indirectly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Reparameterization trick</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Reparameterization_trick.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This gets us to the  final picture of the VAE:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Variational Autoencoder (VAE)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Autoencoder_VAE.png\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To train a VAE:\n",
    "- pass input $\\x^\\ip$ through the Encoder, producing $\\mu^\\ip, \\sigma^\\ip$\n",
    "    - use $\\mu^\\ip, \\sigma^\\ip$ to sample a latent representation $\\z^\\ip$ from the distribution\n",
    "- pass the sampled $\\z^\\ip$ through the decoder, producing $D(\\z^\\ip)$\n",
    "- measure the reconstruction error $\\x^\\ip - D(\\z^\\ip)$, just as in a plain AE\n",
    "- back propagate the error, updating all weights and $\\mu, \\sigma$\n",
    "\n",
    "Each time that we encounter the same training example (e.g., in different epochs), we select another random element from the distribution.\n",
    "\n",
    "Thus the VAE learns to represent the same example from multiple latents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using a VAE to produce synthetic examples\n",
    "\n",
    "\n",
    "\n",
    "We can examine how the latent representations produced by the VAE form clusters:\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "       <th><center>MNIST examples: clustering of latent $\\z$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_latents.png\" width=80%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In comparing the clusterings produced by the VAE against our previous example of a plain Autoencoder be aware\n",
    "- The two models are displaying results on different data: MNIST digits verus Fashion MNIST\n",
    "- The architecture of the Encoder and Decoder are different in the two models\n",
    "    - The plain Autoencoder used extrememly simple architectures\n",
    "        - Could the more complex architecture of the VAE Encoder/Decoder be the cause of tighter clustering ?\n",
    "        \n",
    "Certainly room for experimentation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we interpret the elements of the latent vector ?\n",
    "\n",
    "The latent vector can be thought of as a\n",
    "- reduced dimensionality\n",
    "- representation of $\\x^\\ip$\n",
    "\n",
    "where each element of $\\z^\\ip$ is in\n",
    "- a new basis space\n",
    "- of \"concepts\"\n",
    "- rather than syntactic features\n",
    "\n",
    "This is similar to the ideas we expressed \n",
    "- when trying to interpret Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will attempt an interpretation by  \n",
    "- examining *the neighborhood* around the latent $\\z^\\ip$ of example $\\x^\\ip$\n",
    "- by perturbing each element of the latent vector\n",
    "\n",
    "In our illustration\n",
    "- latent vectors $\\z$ that are implemented as a vectors of length $2$\n",
    "    - as are $\\mu$ and $\\sigma$\n",
    "\n",
    "We vary the first and second element of a vector \n",
    "\n",
    "- and obtain the Decoder output on each combination of first and second element\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the results are *possible* outputs of the Decoder\n",
    "- based on latent vectors $\\z$\n",
    "- that are not necessarily the output of the Encoder on any actual input $\\x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "   <tr>\n",
    "       <th><center>Synthetic MNIST examples from a VAE:<br> Code implements latent $\\z$ as vector with length 2 <br> vary the 2 elements of the latent vector </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_outputgrid.png\" width=90%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can we interpret $\\z_0$ and $\\z_1$ by inspecting how the Decoder output changes\n",
    "by varying each ?\n",
    "\n",
    "- $\\z_0$: controls slant ?\n",
    "    - See the bottom row of $0$'s\n",
    "- $\\z_1$: controls \"verticality\" ?\n",
    "    - See right-most column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional VAE\n",
    "\n",
    "The VAE would seem to offer a solution to the problem of creating synthetic data.\n",
    "\n",
    "But there is a problem\n",
    "- an *unlabeled* example is created\n",
    "- we have no way of knowing the label\n",
    "- nor do we have a way of *controlling* the output so as to be an example with a specified label\n",
    "\n",
    "We can modify the VAE so as to *conditionally* generate an example with a specified label.\n",
    "- [Conditional VAE](Cond_VAE_Generative.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced details\n",
    "\n",
    "## Obtain $\\loss$ by rewriting $\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z|\\x} )$\n",
    "\n",
    "Let's derive a simpler expression for $\\loss$ by manipulating $\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z|\\x})$:\n",
    "\n",
    "$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) &  = & \\sum_z{ \\qrs{\\z|\\x}{\\Phi}(\\log(\\qrs{\\z|\\x}{\\Phi}) - \\log(\\qr{\\z | \\x}) } & \\text{def. of KL} \\\\\n",
    "&  = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log(\\qrs{\\z|\\x}{\\Phi}) - \\log(\\qr{\\z | \\x}) \\right) & \\text{def. of }\\E \\\\\n",
    "&  = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi}) } ( \\; \\log(\\qrs{\\z|\\x}{\\Phi}) \\\\ & & -\\left( \\; \\log( \\pr{\\x | \\z}) + \\log(\\qr{\\z}) - \\log(\\pr{\\x} \\right)    \\,   )  \\;\\;)&  \\text{Bayes theorem on } \\\\\n",
    " & & & \\log(\\qr{\\z|\\x}) \\\\\n",
    "\\KL( \\qrs{\\z|\\x}{\\Phi} \\; ||\\; \\qr{\\z | \\x}) \\\\ - \\log(\\pr{\\x}) & = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\; \\log(\\qrs{\\z|\\x}{\\Phi})  - \\left( \\log( \\pr{\\x | \\z} ) + \\log( \\qr{\\z} ) \\right) \\;\\right) & \\text{ move } \\log(\\pr{\\x}) \\text{ to LHS} \\\\\n",
    " & = & \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\; - \\log( \\pr{\\x | \\z} ) + ( \\; \\log(\\qrs{\\z|\\x}{\\Phi})  - \\log( \\qr{\\z} ) \\; )     \\; \\right) & \\text{re-arrange terms} \\\\\n",
    " & = & - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\pr{\\x | \\z} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi} \\; ||\\;  \\qr{\\z} ) & \\text{def. of KL} \\\\\n",
    " \\loss & = & - \\E_{z \\sim \\qrs{\\z|\\x}{\\Phi} } \\left( \\log( \\pr{\\x | \\z} ) \\right) + \\KL(\\qrs{\\z|\\x}{\\Phi} \\; ||\\;  \\qr{\\z} ) & \\text{since LHS} = \\loss \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The key step**:\n",
    "- Using Bayes Theorem to re-write\n",
    "$$\\log(\\qr{\\z|\\x}) $$\n",
    "as\n",
    "$$\n",
    "\\log( \\pr{\\x | \\z}) + \\log(\\qr{\\z}) - \\log(\\pr{\\x} )\n",
    "$$\n",
    "- This allows us do away with intractable conditional probability $\\qr{\\z|\\x}$\n",
    "- In favor of unconditional probability $\\qr{\\z}$\n",
    "\n",
    "The LHS cannot be optimized via SGD (recall the tractability issue with  $\\qr{\\z|\\x}$).\n",
    "\n",
    "**But the RHS can be made tractable** by\n",
    "- Approximating $\\pr{\\x | \\z}$ with $\\prs{\\x | \\z}{\\Theta}$\n",
    "- Assuming that the distributions $\\qr{\\z}$ (and $\\pr{\\x}$) as Normal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code \n",
    " \n",
    "Here is a diagram of the code for the Encoder side of the VAE.\n",
    "\n",
    "Note that the sub-networks that compute\n",
    "- $\\mu$ (box labeled `z_mean`)\n",
    "- $\\sigma$ (box labeled `z_log_var`)\n",
    "\n",
    "have outputs of length $2$ so  latent vector $\\z$ has two elements.\n",
    "\n",
    "We will use latents of length $2$ in an example below showing\n",
    "some possible outputs of the Decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that\n",
    "- the inputs to the sub-network computing $\\mu$ and $\\sigma$\n",
    "- are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <th><center>VAE: Components (Encoder)</center></th>\n",
    "    <tr>\n",
    "        <td><center><strong>Encoder</bold></strpmg></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_conv_encoder.png\" width=90%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VAE code highlights\n",
    "\n",
    "[Here](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb#scrollTo=HPRXyb06O2y8) is an implementation of the VAE.\n",
    "\n",
    "Highlights\n",
    "- `encoder` samples from the distribution\n",
    "\n",
    "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- custom `train_step`\n",
    "    - computes two-part loss: Reconstruction Loss + KL Loss\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        keras.losses.binary_crossentropy(data, reconstruction),\n",
    "                        axis=(1, 2),\n",
    "                    )\n",
    "                )\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code: VAE, CVAE\n",
    "\n",
    "We can learn much more about the properties and use of a VAE through examples\n",
    "\n",
    "A secondary objective is to look at the code which involves some advanced features of Keras.\n",
    "\n",
    "The architecture of the VAE will be more complex compared to the vanilla Autoencoder.\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "            <th>VAE: Components</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><strong>Encoder</bold></strpmg></td>\n",
    "        <td><center><strong>Decoder</bold></strpmg></td>\n",
    "    <tr>\n",
    "        <td><img src=\"images/vae_conv_encoder.png\" width=90%></td>\n",
    "        <td><img src=\"images/vae_conv_decoder.png\" width=90%></td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Encoder\n",
    "- Note the two branches to nodes `z_mean` and `z_log_var`\n",
    "    - The output of their common parent is used to generate two separate values ($\\mu$ and $\\sigma$)\n",
    "    - $\\mu$ and $\\sigma$ are both vectors of length $2$\n",
    "        - Thus, the sampled $\\z$ is also of length $2$\n",
    "        - In our grid illustration of generating synthetic examples, we vary each of the $2$ components of $\\z$\n",
    "    - Latent is *much* shorter than in the plain VAE\n",
    "        - does the random nature of sampling facilitate shorter representation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore the code through this [notebook](VAE_code.ipynb)\n",
    "- VAE code\n",
    "- CVAE code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function: discussion\n",
    "\n",
    "Let's examine the role of $\\loss_R$ and $\\loss_D$ in the loss function $\\loss$.\n",
    "\n",
    "- What would happen if we dropped $\\loss_D$ ?\n",
    "    - We would wind up with a deterministic $\\z$ and collapse to a vanilla VAE\n",
    "    \n",
    "- What would happen if we dropped $\\loss_R$ ?\n",
    "    - the encoding approximation $\\qrs{\\z|\\x}{\\Phi}$ would be close to the empirical $\\pr{\\z | \\x}$ *in distribution*\n",
    "    - but two variables with the same distribution are not necessarily the same ?\n",
    "        - e.g., get a distribution $p$ by flipping a coin\n",
    "            - let distribution $q$ be a relabelling of $p$ by changing Heads to Tails and vice-versa\n",
    "            - $p$ and $q$ are equal in distribution but clearly different !\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ELBo (Evidence-based Lower Bound)\n",
    "\n",
    "By re-writing the Loss, we removed the intractable term $\\qr{\\z|\\x}$\n",
    "\n",
    "It turns out that even this may not be necessary.\n",
    "\n",
    "For the truly interested reader:\n",
    "- The derivation uses a method known as *Variational Inference*.  See this \n",
    "[blog](https://mbernste.github.io/posts/variational_inference/) for a summary.\n",
    "- One can show that loss $\\loss$ is equal to $-1$ times the *ELBo* (Evidence Based Lower Bound)\n",
    "\n",
    "So if one knows how to maximize the [ELBo](https://mbernste.github.io/posts/elbo/), one can minimize the loss.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
