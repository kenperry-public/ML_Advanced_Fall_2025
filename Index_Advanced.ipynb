{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<html>\n",
    "<p style=\"font-size:32px\"><strong>Classical Machine Learning</strong></p>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<p style=\"font-size:26px\"><strong>Week 0</strong></p>\n",
    "</html>\n",
    " \n",
    "\n",
    "**Plan**\n",
    "- Setting up your learning and programming environment\n",
    "\n",
    "\n",
    "**Getting started**\n",
    "- [Setting up your ML environment](Setup_NYU.ipynb)\n",
    "    - [Choosing an ML environment](Choosing_an_ML_Environment_NYU.ipynb)\n",
    "- [Quick intro to the tools](Getting_Started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 1\n",
    "\n",
    "**Plan**\n",
    "\n",
    "We give a brief introduction to the course.\n",
    "\n",
    "We then present the key concepts that form the basis for this course\n",
    "- For some: this will be review\n",
    "\n",
    "\n",
    "## Intro to Advanced Course\n",
    "\n",
    "- [Introduction to Advanced Course](Intro_Advanced.ipynb)\n",
    "\n",
    "\n",
    "## Using an AI Assistant as a Personal Tutor\n",
    "\n",
    "Knowledge of Deep Learning is a prerequisite for this course.  \n",
    "\n",
    "For those of you who need a review,\n",
    "we will do so at a **very rapid pace and abbreviated manner**.\n",
    "\n",
    "But using AI Assistants (e.g., ChatGPT) can be a great resource for getting you up to speed.\n",
    "\n",
    "The key is to using them as a personal tutor.  Some advice\n",
    "- describe the role you want them to play (e.g., Professor, practitioner)\n",
    "    - this sets the level for depth of knowledge it will convey\n",
    "- describe your level of knowledge (Graduate student, undergraduate, hacker)\n",
    "    - this sets the level of complexity for the responses it provides\n",
    "- Treat it as a tutor\n",
    "    - ask for a concept to be explained, at varying levels of depth\n",
    "    - ask follow-up questions until you get what you need\n",
    "    \n",
    "[Here](https://www.perplexity.ai/search/you-are-an-expert-in-deep-lear-qPHF81eAScGyAJogE5idCw?0=)\n",
    "is an example of such a conversation using Perplexity as the AI Assistant\n",
    "- you may use any Assistant that you prefer: they are very similar in capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Review/Preview of concepts from Intro Course (**very** abbreviated)\n",
    "Here is a *quick reference* of key concepts/notations from the Intro course\n",
    "- For some: it will be a review, for others: it will be a preview.  \n",
    "- We will devote a sub-module of this lecture to elaborate on each topic in slightly more depth.\n",
    "    - For a more detailed explanation: please refer to the material from the Intro course ([repo](https://github.com/kenperry-public/ML_Spring_2023))\n",
    "    \n",
    "- [Review and Preview](Review_Advanced.ipynb)\n",
    "\n",
    "\n",
    "You may want to run your code on Google Colab in order to take advantage of powerful GPU's.\n",
    "\n",
    "Here are some useful tips:\n",
    "\n",
    "[Google Colab tricks](Colab_practical.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Transformers: Review\n",
    "\n",
    "**Preview**\n",
    "\n",
    "There is lots of interest in Large Language Models (e.g., ChatGPT).  These are based on an architecture called the Transformer.  We will introduce the Transformer and demonstrate some amazing results achieved by using Transformers to create Large Language Models.\n",
    "\n",
    "Attention is a mechanism that is a core part of the Transformer.  We will begin by first introducing Attention.\n",
    "\n",
    "We will then take a detour and study the Functional model architecture of Keras.  Unlike the Sequential model, which is an ordered sequence of Layers, the organization of blocks in a Functional model is more general.  The Advanced architectures (e.g., the Transformer) are built using the Functional model.\n",
    "\n",
    "Once we understand the technical prerequisites, we will examine the code for the Transformer.\n",
    "\n",
    "[Transformers: Review](Review_Transformer.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "- Attention\n",
    "    - [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)  \n",
    "- Transfer Learning    \n",
    "    - [Sebastian Ruder: Transfer Learning](https://ruder.io/transfer-learning/)\n",
    "- HuggingFace course\n",
    "    - [Transformers: concepts](https://huggingface.co/learn/nlp-course/chapter1/4?fw=pt)\n",
    "    \n",
    "**Further reading**\n",
    "- Attention\n",
    "    - [Neural Machine Translation by Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "    - Geron Chapter 16\n",
    "    - [An Analysis of BERT's Attention](https://arxiv.org/pdf/1906.04341.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The remaining \"reviews\" will be omitted in class**\n",
    "- we will reference them in passing when dealing with the related topic in future lectures\n",
    "\n",
    "\n",
    "### [Transformer: flavors](Transformer.ipynb#Transformer-variants)\n",
    "\n",
    "**Suggested reading**\n",
    "- HuggingFace course\n",
    "    - [Transformer styles](https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt)\n",
    "\n",
    "\n",
    "\n",
    "### Attention: in depth\n",
    "\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "\n",
    "### [Transfer Learning: Review ](Review_TransferLearning.ipynb)\n",
    "\n",
    "### [Natural Language Processing: Review](Review_NLP.ipynb)\n",
    "\n",
    "### [LLM: Review](Review_LLM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Week 2: Technical tools/background\n",
    "\n",
    "## Functional Models\n",
    "\n",
    "    \n",
    "**Plan**\n",
    "\n",
    "Enough theory (for the moment) !\n",
    "\n",
    "The Transformer (whose theory we have presented) is built from plain Keras.\n",
    "\n",
    "Our goal is to dig into the **code** for the Transformer so that you too will learn how to build advanced models.\n",
    "\n",
    "Before we can do this, we must\n",
    "- go beyond the Sequential model of Keras: introduction to the Functional model\n",
    "- understand more \"advanced\" features of Keras: customomizing layers,  training loops, loss functions\n",
    "- The Datasets API\n",
    "\n",
    "**Basics**\n",
    "\n",
    "We start with the basics of Functional models, and will give a coding example of such a model in Finance.\n",
    "\n",
    "- [Functional API](Functional_Models.ipynb)\n",
    "\n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "- Keras docs\n",
    "    - [Functional API](https://keras.io/guides/functional_api/)\n",
    "    - [Making new layers and models via sub-classing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Functional Model Code:  A Functional model in Finance: \"Factor model\"\n",
    "\n",
    "\n",
    "\n",
    "We illustrate the basic features of Functional models with an example\n",
    "- does not use the additional techniques of the next section (Advanced Keras)\n",
    "\n",
    "[Autoencoders for Conditional Risk Factors](Autoencoder_for_conditional_risk_factors.ipynb)\n",
    "- [code](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/20_autoencoders_for_conditional_risk_factors/06_conditional_autoencoder_for_asset_pricing_model.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "- [Autoencoder asset pricing models](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3335536)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting it all together: Code: the Transformer (continued)\n",
    "\n",
    "\n",
    "**Plan**\n",
    "\n",
    "With the base of advanced Keras under our belts, it's time to understand the Transformer, in code\n",
    "\n",
    "We will examine the code in the excellent [TensorFlow tutorial on the Transformer](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "- more in-depth than our presentation\n",
    "- more background\n",
    "\n",
    "The tutorial is especially recommended for those without the basics of the Transformer from my Intro course\n",
    "\n",
    "- [The Transformer: Understanding the Pieces](Transformer_Understanding_the_Pieces.ipynb)\n",
    "- [The Transformer: Code](Transformer_code.ipynb)\n",
    "- [Implementing Attention: detail](Implementing_Attention.ipynb)\n",
    "- [Choosing a Transformer architecture](Transformer_Choosing_a_PreTrained_Model.ipynb)\n",
    "\n",
    "**Suggested reading**\n",
    "\n",
    "There is an excellent tutorial on Attention and the Transformer which I recommend:\n",
    "- [Tensorflow tutorial: Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)\n",
    "\n",
    "\n",
    "**Deeper dive**\n",
    "- [Implementing Attention](Attention_Lookup.ipynb)\n",
    "- [Residual connections](RNN_Residual_Networks.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional Deep Learning resources\n",
    "\n",
    "Here are some resources that I have found very useful.\n",
    "\n",
    "Some of them are very nitty-gritty, deep-in-the-weeds (even the \"introductory\" courses)\n",
    "- For example: let's make believe PyTorch (or Keras/TensorFlow) didn't exists; let's invent Deep Learning without it !\n",
    "    - You will gain a deeper appreciation and understanding by re-inventing that which you take for granted\n",
    "    \n",
    "\n",
    "## [Andrej Karpathy course: Neural Networks, Zero to Hero](https://karpathy.ai/zero-to-hero.html)\n",
    "- PyTorch\n",
    "- Introductory, but at a very deep level of understanding\n",
    "    - you will get very deep into the weeds (hand-coding gradients !) but develop a deeper appreciation\n",
    "    \n",
    "## fast.ai\n",
    "\n",
    "`fast.ai` is a web-site with free courses from Jeremy Howard.\n",
    "- PyTorch\n",
    "- Introductory and courses \"for coders\"\n",
    "- Same courses offered every few years, but sufficiently different so as to make it worthwhile to repeat the course !\n",
    "    - [Practical Deep Learning](https://course.fast.ai/)\n",
    "    - [Stable diffusion](https://course.fast.ai/Lessons/part2.html)\n",
    "        - Very detailed, nitty-gritty details (like Karpathy) that will give you a deeper appreciation\n",
    "        \n",
    "## [Stefan Jansen: Machine Learning for Trading](https://github.com/stefan-jansen/machine-learning-for-trading)\n",
    "\n",
    "An excellent github repo with notebooks\n",
    "- using Deep Learning for trading\n",
    "- Keras\n",
    "- many notebooks are cleaner implementations of published models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignments\n",
    "\n",
    "Your assignments should follow the [Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.547px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
