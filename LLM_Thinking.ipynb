{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reasoning traces/Chain of Thought (CoT)\n",
    "\n",
    "**Review**\n",
    "\n",
    "We begin with a quick review of Chain of Thought.\n",
    "\n",
    "- [Chain of Thought](NLP_Beyond_LLM.ipynb#Chain-of-thought-prompting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally:\n",
    "\n",
    "Consider a task \n",
    "- described by prompt $\\x$ (a sequence)\n",
    "- with response $\\y$ (a sequence)\n",
    "\n",
    "The most direct solution of the task can be described by the following sequence\n",
    "describing the LLM's computation\n",
    "\n",
    "$$\n",
    "\\x,  \\y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the task is sufficiently complicated\n",
    "- e.g., is best solved by multi-step reasoning\n",
    "\n",
    "it has been shown that the chances of generating a better $\\hat\\y$ (sequence) answer\n",
    "are improved by \"Chain of Thought\" reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**CoT summary**\n",
    "\n",
    "Given prompt\n",
    "$$\n",
    "\\x_{(1:\\bar T)}\n",
    "$$\n",
    "rather than *immediately* producing response\n",
    "$$\n",
    "\\y_{(1: T)}\n",
    "$$\n",
    "giving computation trace\n",
    "$$\n",
    "\\x_{(1:\\bar T)},  \\y_{(1: T)}\n",
    "$$\n",
    "\n",
    "an LLM is trained to think \"Step by Step\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the chart below, compare\n",
    "- Non Chain of thought: left side\n",
    "- Chain of thought reasoning: right side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Chain of Thought Prompting</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/cot_prompt_example.png\" width=80%>\n",
    "    </tr>\n",
    "    \n",
    "    Attribution: https://arxiv.org/pdf/2201.11903.pdf\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By thinking \"step by step\"\n",
    "- the same LLM\n",
    "- produces a correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"Step by step\" thinking\n",
    "- creates a sequence of steps\n",
    "- the Chain of Thought (\"reasoning\" trace) $\\rat$\n",
    "- enumerating sequential steps of a process that produces the response\n",
    "$$\n",
    "    \\rat =  [ \\rat_{(1)}, \\ldots, \\rat_{(\\text{num_thoughts})} ]\n",
    "$$\n",
    "- where each $\\rat_\\tp$ is a thought represented as multi-token sequence \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally, the LLM's \"thought process\" can be represented as a \n",
    "concatenation \n",
    "$$\n",
    "\\x, \\rat, \\y\n",
    "$$\n",
    "of\n",
    "- prompt $\\x$\n",
    "- the *reasoning trace* $\\rat$\n",
    "    - a sequence of *thoughts* ordered linearly: the Chain of Thought\n",
    "    \n",
    "    $$\n",
    "    \\rat =  [ \\rat_{(1)}, \\ldots, \\rat_{(\\text{num_thoughts})} ]\n",
    "    $$\n",
    "    \n",
    "\n",
    "- response $\\y$\n",
    "\n",
    "\n",
    "Note that each thought $\\rat_\\tp$ is a sequence of tokens.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the model computes response $\\y$ as\n",
    "$$\n",
    "\\pr{\\y\\, | \\x ,  \\rat }\n",
    "$$\n",
    "rather than directly as\n",
    "$$\n",
    "\\pr{\\y \\, | \\x }\n",
    "$$\n",
    "\n",
    "The reasoning trace $\\rat$\n",
    "- *conditions* $\\y$\n",
    "- on the reasoning steps\n",
    "- improving Performance\n",
    "\n",
    "It is the conditioning on the reasoning step that results in more desirable responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A model that reasons: \"Think before speaking\"\n",
    "\n",
    "Rather than immediately outputting the \"final\" response\n",
    "- Chain of Thought\n",
    "- gives the model the ability to \"think\"\n",
    "    - create reasoning traces\n",
    "- before \"speaking\" the final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But reasoning can entail more than just a plan that leads to a single \"final\" answer.\n",
    "\n",
    "It may also encompass *solution strategies\"\n",
    "- *reflection* \n",
    "    - \"reflect on\" (i.e., evaluate)\n",
    "    - intermediate solutions (reasoning traces that have not yet created an answer)\n",
    "    - to determine whether the reasoning trace is on \"the right track\" to a good solution\n",
    "    \n",
    "- *revision*\n",
    "    - abandon an intermediate solution\n",
    "    - change the intermediate solution and extend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "By convention, thoughts are bracketed by the \"think\" delimiter\n",
    "\n",
    "$$\n",
    "\\begin{array} \\\\\n",
    "\\bthink \\\\\n",
    "\\text{Thinking step-by-step, the first step is }\\ldots \\\\\n",
    "\\ethink\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using this bracketing\n",
    "- allows the model to behavior differently\n",
    "    - when \"thinking\"\n",
    "    - versus \"speaking\"\n",
    "    \n",
    "We will see this in a module on Reasoning in Latent Space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bracketing can also be used\n",
    "- in combination with a user-controlled toggle\n",
    "- to suppress output of the reasoning trace \n",
    "\n",
    "when the user only wants to see the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Reasoning model in action\n",
    "\n",
    "Here is an example.\n",
    "\n",
    "The prompt is\n",
    "\n",
    "    Prove to me that the residuals of Linear Regression are not correlated with the independent variables\n",
    "    \n",
    "Note\n",
    "- the reasoning trace is truncated (ran out of credits on HF before I could write the entire file)\n",
    "- output: highlight the first words of each paragraph\n",
    "    - indicates\n",
    "        - thinking step by step\n",
    "        - revision, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<!--- #include (r1_prove_residuals_orthogonal_full.txt) --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def print_file_content(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            print(line)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file '{filename}' was not found.\"\n",
    "    except IOError:\n",
    "        return f\"Error: There was an issue reading the file '{filename}'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "**Okay**, so I need to prove that the residuals in linear regression aren't correlated with the independent variables. Hmm, let me start by recalling what residuals are. They're the differences between the observed values of the dependent variable and the values predicted by the regression model. \n",
      "**Right**, so residuals = observed Y - predicted Y.\n",
      "\n",
      "Now, in linear regression, we have the model Y = Xβ + ε, where X is the matrix of independent variables, β is the coefficient vector, and ε is the error term. The residuals are estimates of these errors, right? \n",
      "**But** wait, the actual errors ε might have certain properties, like being uncorrelated with X if the model is correctly specified. \n",
      "**But** the residuals are different because they're calculated after estimating β. So maybe there's something in the estimation process that ensures residuals are uncorrelated with X?\n",
      "\n",
      "\n",
      "**Let me think** about the ordinary least squares (OLS) method. The OLS estimator minimizes the sum of squared residuals. The first-order conditions for this minimization might impose some orthogonality between the residuals and the independent variables. Oh, right! The normal equations in OLS state that X'(Y - Xβ_hat) = 0, where β_hat is the OLS estimator. That equation implies that the transpose of the X matrix multiplied by the residuals is zero. \n",
      "\n",
      "Breaking that down, each column of X (each independent variable) is multiplied by the residuals and summed up, resulting in zero. So, the dot product of each independent variable with the residuals is zero. In statistical terms, that means the sample covariance between each independent variable and the residuals is zero, right? Because covariance is related to the dot product after centering the variables. \n",
      "\n",
      "\n",
      "**Wait**, but if the variables are centered (mean zero), then the covariance is just the expectation of the product. \n",
      "**But** even if they aren't centered, the normal equations still give X' residuals = 0. So, in the sample, the residuals are orthogonal to each of the independent variables. Therefore, their correlation should be zero. \n",
      "\n",
      "\n",
      "**But** is this always true? Let me check with a simple example. Suppose I have one independent variable x and a dependent variable y. The OLS regression line is y = β0 + β1 x. The residuals are e_i = y_i - (β0 + β1 x_i). The normal equations for this case are:\n",
      "\n",
      "1. The sum of residuals is zero: Σe_i = 0.\n",
      "2. The sum of residuals multiplied by x_i is zero: Σe_i x_i = 0.\n",
      "\n",
      "The second equation directly implies that the sample covariance between x and e is zero because covariance is [Σ(x_i - x̄)(e_i - ē)] / (n-1). \n",
      "**But** since Σe_i = 0, ē = 0, and Σx_i e_i = 0, so covariance is [Σx_i e_i - x̄ Σe_i] / (n-1) = 0. Therefore, the sample covariance is zero, which means the correlation is also zero. \n",
      "\n",
      "So in the simple linear regression case, the residuals are uncorrelated with the independent variable. Extending this to multiple regression, each independent variable's covariance with the residuals is zero because of the normal equations. Therefore, all the correlations between residuals and each X variable are zero. \n",
      "\n",
      "\n",
      "**But** wait, does this hold in the population or just in the sample? The OLS properties are about the sample, but if the model is correctly specified, the error term ε is uncorrelated with X in the population. However, the residuals are estimates of ε and have different properties. \n",
      "**But** regardless of the population, in the sample, the residuals are constructed to be uncorrelated with X. \n",
      "\n",
      "Another way to see this is through the Frisch-Waugh-Lovell theorem. If you regress Y on X and get residuals e, then those residuals are orthogonal to the space spanned by X. Therefore, any variable in X cannot explain any variation in e, which implies zero correlation. \n",
      "\n",
      "\n",
      "**But** maybe there's a case where this doesn't hold? For example, if we don't include an intercept term? \n",
      "**Wait**, if the model doesn't have an intercept, then the sum of residuals isn't necessarily zero. \n",
      "**But** even then, the normal equations still require that X' e = 0. So even without an intercept, each column of X (including a constant column if present) is orthogonal to e. So if there's no intercept, the first column of X is all ones, and the normal equation would require that Σe_i = 0, but if there's no intercept, then that column isn't included. \n",
      "**Wait**, no. If there's no intercept, then X doesn't have a column of ones. Then the normal equations would just be X' e = 0, where X is the matrix without the intercept. So in that case, the residuals are orthogonal to each of the independent variables included in the model, but the sum of residuals might not be zero. However, the correlation between residuals and each X variable would still be zero because their covariance is zero. \n",
      "\n",
      "\n",
      "**Wait**, but if the variables aren't centered, does that affect the correlation? Correlation is covariance divided by the product of standard deviations. If covariance is zero, correlation is zero regardless of the scaling. So even if variables aren't centered, zero covariance implies zero correlation. \n",
      "\n",
      "Therefore, in OLS, whether or not there's an intercept, the residuals are uncorrelated with each independent variable in the model. However, if the model is misspecified, say omitting a variable, then the residuals might be correlated with the omitted variable, but not with the included ones. \n",
      "\n",
      "So putting this all together, the key point is that the normal equations from OLS ensure that the residuals are orthogonal to the space spanned by X, which includes each independent variable. Therefore, the sample covariance (and hence correlation) between each X and the residuals is zero. \n",
      "\n",
      "I think that covers it. Let me just verify with a textbook or notes. Yes, in the OLS derivation, the normal equations X'e = 0 are crucial, and they directly lead to the orthogonality between residuals and regressors. Therefore, the residuals are uncorrelated with the independent variables in the sample. This is a property of the OLS estimation procedure, ensuring that all the explanatory power of X has been accounted for in the model, leaving the residuals uncorrelated with X.\n",
      "</think>\n",
      "\n",
      "To prove that the residuals of a linear regression are uncorrelated with the independent variables, we rely on the properties of Ordinary Least Squares (OLS) estimation. Here's a step-by-step explanation:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Model Setup**\n",
      "Consider the linear regression model:\n",
      "\\[\n",
      "\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon},\n",
      "\\]\n",
      "where:\n",
      "- \\(\\mathbf{Y}\\) is the dependent variable,\n",
      "- \\(\\mathbf{X}\\) is the \\(n \\times k\\) matrix of independent variables (including a constant term if applicable),\n",
      "- \\(\\boldsymbol{\\beta}\\) is the vector of coefficients,\n",
      "- \\(\\boldsymbol{\\varepsilon}\\) is the error term.\n",
      "\n",
      "The OLS estimator \\(\\hat{\\boldsymbol{\\beta}}\\) minimizes the sum of squared residuals:\n",
      "\\[\n",
      "\\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\left(Y_i - \\mathbf{X}_i\\boldsymbol{\\beta}\\right)^2.\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Normal Equations**\n",
      "The first-order conditions for minimizing the sum of squared residuals lead to the **normal equations**:\n",
      "\\[\n",
      "\\mathbf{X}^\\top (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}.\n",
      "\\]\n",
      "Let \\(\\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) denote the residuals. The normal equations simplify to:\n",
      "\\[\n",
      "\\mathbf{X}^\\top \\mathbf{e} = \\mathbf{0}.\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Interpretation of \\(\\mathbf{X}^\\top \\mathbf{e} = \\mathbf{0}\\)**\n",
      "This equation implies:\n",
      "- For each column of \\(\\mathbf{X}\\) (i.e., each independent variable), the dot product with the residuals \\(\\mathbf{e}\\) is zero:\n",
      "\\[\n",
      "\\sum_{i=1}^n X_{ji} e_i = 0 \\quad \\text{for all } j = 1, 2, ..., k.\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Covariance and Correlation**\n",
      "The sample covariance between an independent variable \\(X_j\\) and the residuals \\(\\mathbf{e}\\) is:\n",
      "\\[\n",
      "\\text{Cov}(X_j, \\mathbf{e}) = \\frac{1}{n-1} \\sum_{i=1}^n (X_{ji} - \\bar{X}_j)(e_i - \\bar{e}).\n",
      "\\]\n",
      "From the normal equations:\n",
      "- \\(\\sum_{i=1}^n X_{ji} e_i = 0\\),\n",
      "- If the model includes an intercept, \\(\\sum_{i=1}^n e_i = 0\\) (so \\(\\bar{e} = 0\\)).\n",
      "\n",
      "Thus:\n",
      "\\[\n",
      "\\text{Cov}(X_j, \\mathbf{e}) = \\frac{1}{n-1} \\left(\\sum_{i=1}^n X_{ji} e_i - \\bar{X}_j \\sum_{i=1}^n e_i \\right) = 0.\n",
      "\\]\n",
      "Since covariance is zero, the **correlation** between \\(X_j\\) and \\(\\mathbf{e}\\) is also zero:\n",
      "\\[\n",
      "\\text{Corr}(X_j, \\mathbf{e}) = \\frac{\\text{Cov}(X_j, \\mathbf{e})}{\\sigma_{X_j} \\sigma_{\\mathbf{e}}} = 0.\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Geometric Interpretation**\n",
      "The residuals \\(\\mathbf{e}\\) are orthogonal to the column space of \\(\\mathbf{X}\\) by construction. This means \\(\\mathbf{e}\\) contains no information that can be explained by \\(\\mathbf{X}\\), ensuring zero correlation.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Conclusion**\n",
      "The OLS estimation procedure ensures that:\n",
      "- Residuals are **uncorrelated** with all independent variables in \\(\\mathbf{X}\\),\n",
      "- This holds **by design** due to the normal equations \\(\\mathbf{X}^\\top \\mathbf{e} = \\mathbf{0}\\).\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary**\n",
      "In linear regression, the residuals are guaranteed to be uncorrelated with the independent variables because the OLS estimator explicitly enforces orthogonality between \\(\\mathbf{X}\\) and \\(\\mathbf{e}\\). This is a fundamental property of OLS and ensures that all explanatory power of \\(\\mathbf{X}\\) is exhausted in the model.\n"
     ]
    }
   ],
   "source": [
    "r1_out_file = 'r1_prove_residuals_orthogonal_full.txt'\n",
    "print_file_content(r1_out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I have highlighted (with double stars \"**\") key moment in the trace\n",
    "- **Okay**\n",
    "    - model makes a plan\n",
    "        - recall definitions\n",
    "- **Right**\n",
    "    - reflection and confirmation that we are on the right track\n",
    "-  **Now** ... right ?\n",
    "    - reflection, perhaps leading to revision ?\n",
    "- **Is that part of the Gauss-Markov assumptions? Yeah**\n",
    "    - reflection\n",
    "- **Wait**\n",
    "    - revision\n",
    "- **But wait**\n",
    "    - revision\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thinking harder\n",
    "\n",
    "The reasoning trace $\\rat$ is a sequences of thoughts.\n",
    "\n",
    "How long should a model \"think\" ?\n",
    "- what is the length (measured in tokens of  the reasoning traces $\\rat$ ?\n",
    "\n",
    "The initial approaches to \"reasoning\" favored\n",
    "- long thoughts (thinking \"harder\" or \"deeper\")\n",
    "\n",
    "So one approach to a better Reasoner is getting it to produce longer reasoning traces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thinking budget\n",
    "\n",
    "**Reference**\n",
    "\n",
    "[s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393)\n",
    "\n",
    "An [interesting approach](https://arxiv.org/pdf/2501.19393) is to force a Reasoner\n",
    "to obey a thinking budget for its reasoning traces\n",
    "- measured in number of thoughts, or tokens\n",
    "    - minimum\n",
    "    - maximum\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This approach uses [Test-time Scaling](Test_time_compute.ipynb#Test-time-Scaling-Law)\n",
    "\n",
    "Rather than training the Reasoner to obey a budget\n",
    "- this approach **modifies the Inference loop**\n",
    "    - the loop enforcing the auto-regressive behavior of the LLM\n",
    "- to \n",
    "    - force the model to continue inference if the reasoning trace is below budget\n",
    "    - truncate inference when the budget is exceeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Forcing an overly long reasoning trace to adhere to maximum length is straight-forward\n",
    "- Truncate the trace\n",
    "- Insert an `<eos>` token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the reasoning trace produced is too short\n",
    "- the modified inference loop\n",
    "    - replaces the token\n",
    "    - with a token sequence that causes the model to revise/extend the reasoning trace\n",
    "        - e.g., \"Alternatively\", \"Wait\", \"But\"\n",
    "    - thus causing the LLM to continue \"thinking\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is what the result looks like:\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <center><strong>Budget Forcing</strong></center>\n",
    "    <tr>\n",
    "        <img src=\"images/budget_forcing.png\">\n",
    "    </tr>\n",
    "    \n",
    "Attribution: https://arxiv.org/pdf/2501.19393#page=4\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code for enforcing a thinking budget\n",
    "\n",
    "Here is some simplified code\n",
    "- derived from the paper's [Github](https://github.com/simplescaling/s1/tree/main?tab=readme-ov-file#vllm-with-budget-forcing)\n",
    "- note some minor formating differences\n",
    "    - does not explicitly use `<think>` and `</think>` to denote reasoning trace\n",
    "    - instead, uses `<|im_start|>think`\n",
    "        - denotes this part of the \"conversation\" is \"thinking mode\"\n",
    "        \n",
    "Note\n",
    "- the multiple uses of the LLM `generate` method\n",
    "- to generate the next part of the output\n",
    "    - auto-regressive loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>\n",
    "        Code: Budget Forcing\n",
    "        <br>\n",
    "        creating the reasoning trace\n",
    "        </strong></center>\n",
    "    \n",
    "    # Constants  \n",
    "    ignore_str = \"Wait\"\n",
    "    max_tokens_thinking_tmp = MAX_TOKENS_THINKING\n",
    "\n",
    "    # Generate the start of the reasoning trace: Change the Assistant's role to **think**\n",
    "    prompt += \"<|im_start|>think\"\n",
    "    o = model.generate(\n",
    "        prompt,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "     \n",
    "    # Increase length of reasoning trace until length is at least MAX_TOKENS_THINKING\n",
    "    if max_tokens_thinking_tmp > 0:\n",
    "        for i in range(NUM_IGNORE): # Num of times to skip stop token\n",
    "            # Append the last extension to the reasoning trace\n",
    "            prompt += o[0].outputs[0].text\n",
    "            \n",
    "            # Insert a \"Wait\"\n",
    "            prompt += ignore_str\n",
    "            \n",
    "            # Generate the next extension of the trace\n",
    "            o = model.generate(\n",
    "                prompt,\n",
    "                sampling_params=sampling_params\n",
    "            )\n",
    "            \n",
    "            # Reduce the remaining number of thinking tokens to generate\n",
    "            max_tokens_thinking_tmp -= len(o[0].outputs[0].token_ids)\n",
    "            \n",
    "            \n",
    "            ...\n",
    "            \n",
    "          \n",
    "            \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>\n",
    "        Code: Budget Forcing\n",
    "        <br>\n",
    "        creating the final answer\n",
    "        </strong></center>\n",
    "    \n",
    "    ### Final answer ###\n",
    "    # Append the last extension to the reasoning trace\n",
    "    prompt += o[0].outputs[0].text # You can also append \"Final Answer:\" here like we do for some evaluations \n",
    "                                   # to prevent the model from just continuing to reason in its answer\n",
    "                                   # when early exiting\n",
    "    \n",
    "    # Create the \"answer\", which follows the reasoning trace\n",
    "    ...\n",
    "    \n",
    "    o = model.generate(\n",
    "        prompt,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    print(\"With budget forcing:\") # You will see that after the \"Wait\" in the reasoning trace it fixes its answer\n",
    "    print(prompt + o[0].outputs[0].text)\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Smarter not longer\n",
    "\n",
    "At first glance\n",
    "- longer reasoning trace should be preferred to a shorter trace\n",
    "    - \"deeper\" reasoning\n",
    "\n",
    "There is some empirical evidence that shows\n",
    "- first preliminary response *can* often lead to correct response\n",
    "- **but** the initial reasoning trace is often prematurely abandoned\n",
    "    - the model \"under-thinks\" and tries something else if the first approach continues for too long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why does a potentially successful initial reasoning trace get abandoned by the Reasoner ?\n",
    "\n",
    "Perhaps it is in the training dataset\n",
    "- hard problems in the training set have long but unsuccessful reasoning traces\n",
    "- the model learns to abandon long traces\n",
    "\n",
    "The problem is the inability to distinguish between\n",
    "- long traces of hard problems that fail to be solved\n",
    "- long traces that are needed for less-hard, solvable problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to think smarter\n",
    "\n",
    "One approach is\n",
    "- train a model to estimate the difficulty of a give task\n",
    "- have the Reasoner adapt its test-time compute based on the difficulty\n",
    "    - more/longer thoughts for harder tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively\n",
    "- use a *Process Reward Model*\n",
    "    - train a Reward Model to estimate\n",
    "        - whether each step in the thought is advancing toward a good response\n",
    "    - continue a thought only if the estimated reward is high\n",
    "    \n",
    "The training dataset for a Process Reward Model\n",
    "- is expensive\n",
    "- human labeling of the steps and rewards\n",
    "    - reward limited to categorical (Positive/Negative/Neutral) versus continuous values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training an LLM to reason\n",
    "\n",
    "Basic Chain of Thought reasoning seems to be a property of models\n",
    "- that emerges once models grow in size\n",
    "- without explicit pre-training\n",
    "\n",
    "Still, models need to be encouraged to employ Chain of Thought\n",
    "- by appending \"Let's think step by step\"\n",
    "    - zero-shot prompting, no exemplars demonstrating the chain of thought\n",
    "- to the prompt\n",
    "\n",
    "The exact phrase (e.g., \"Let's think step by step\") that best elicits this behavior\n",
    "is the subject of Prompt Engineering\n",
    "- the module on [Automated Prompt Engineer](Prompt_Engineering_APE.ipynb#Zero-shot:-Improving-on-%22Let's-think-step-by-step%22)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to produce an LLM that \"reasons\"\n",
    "better than basic Chain of Thought\n",
    "- e.g., reflecting and revising\n",
    "    \n",
    "we need to instill this behavior by fine-tuning a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given prompt $\\x$\n",
    "- we fine-tune a model to produce responses\n",
    "$$\\rat, \\y$$\n",
    "- rather than the direct responses\n",
    "$$\\y$$\n",
    "\n",
    "by fine-tuning it on examples of the form\n",
    "\n",
    "$$\n",
    "\\x, \\rat, \\y\n",
    "$$\n",
    "- rather than\n",
    "$$\n",
    "\\x,  \\y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The cost of producing examples with reasoning traces \n",
    "$$\n",
    "\\x, \\rat, \\y\n",
    "$$\n",
    "can be substantial.\n",
    "\n",
    "Often, it involves asking a human to adapt a training set with examples\n",
    "$$\n",
    "\\x, \\y\n",
    "$$\n",
    "by creating the reasoning traces $\\rat$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reducing the cost of creating examples with reasoning traces\n",
    "\n",
    "Removing the human from the training data production process is\n",
    "highly desirable (cost reduction).\n",
    "\n",
    "Boot-strapping a training dataset from an existing model is one solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One method of boot-strapping:\n",
    "\n",
    "Ask a strong, non-reasoning LLM $\\model^\\text{non-reasoner}$ to solve task\n",
    "- given $\\x^\\ip$\n",
    "- \"think step by step\" (create $\\rat^\\ip$)\n",
    "- produce response $\\y^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "that Chain of Thought (\"think step by step\") reasoning\n",
    "- seems to emerge from the natural training examples used to train $\\model^\\text{non-reasoner}$ \n",
    "- does *not* require explicit training examples with reasoning traces\n",
    "- hence, the boot-strapping process is well grounded\n",
    "    - non-circular: the first model is trained without reasoning traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively:\n",
    "\n",
    "Ask an existing reasoning model $\\model^\\text{reasoner}$ to solve task\n",
    "- given $\\x^\\ip$\n",
    "- produce response $\\rat^\\ip, \\y^\\ip$\n",
    "\n",
    "This effectively bootstraps from existing reasoner $\\model^\\text{reasoner}$\n",
    "- base case: *someone* needs to (manually) create the dataset to train the first reasoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both these approaches creates a machine-generated example\n",
    "    $$\\langle \\x^\\ip, \\rat^\\ip, \\y^\\ip \\rangle$$\n",
    "that can be used to train a new, reasoning LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-improvement\n",
    "\n",
    "We can use this boot-strapping process iteratively to\n",
    "create a sequence of reasoners with increases strength.\n",
    "\n",
    "See [LLM Self-Improvement](LLM_Self_Improvement.ipynb) for the technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The iterative process of Self-Improvement\n",
    "- trains the target model $\\model$ in stages\n",
    "    - creating a sequence of fine-tuned models $\\model_{(0)}, \\model_{(1)}, \\dots$\n",
    "    - of reasoners of increasing power\n",
    "- base case\n",
    "    - fine-tune non-reasoning initial $\\model_{(0)}$\n",
    "    - use weak reasoning examples\n",
    "        - from Chain of Thought or a weak reasoning model\n",
    "    - resulting in weak reasoner $\\model_{(1)}$\n",
    "- inductive case\n",
    "    - create improved $\\model_{(\\tt+1)}$\n",
    "    - by using reasoning traces created by $\\model_\\tp$\n",
    "    - for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distillation\n",
    "\n",
    "Using an existing reasoning model $\\model^\\text{reasoner}$\n",
    "- to instill reasoning (by fine-tuning)in a non-reasoning model \n",
    "- with examples of reasoning traces created by $\\model^\\text{reasoner}$\n",
    "- as we did above\n",
    "\n",
    "is an illustration of the technique \n",
    " called *Distillation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given a model $\\model^{\\text{student}}$ that has *not yet been fine-tuned* for the Target task T\n",
    "- we adapt $\\model^{\\text{student}}$ into model that solves the target task T\n",
    "- by fine-tuning it on examples $\\langle \\dot\\X, \\dot\\y \\rangle $ of the target task's input/output relationship\n",
    "    - creating a feature vector $\\dot\\x^\\ip$ for the Target task\n",
    "    - using $\\model^\\text{teacher}$ \n",
    "    to create the response $\\dot\\y^\\ip$\n",
    "    - for $1 \\le i \\le m$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that \n",
    "- $\\model^\\text{student}$ does not directly learn the task\n",
    "- it only learns to mimic $\\model^\\text{teacher}$\n",
    "- inheriting all the flaws and limitations of the teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reducing the cost of running a reasoning model\n",
    "\n",
    "Often\n",
    "- $\\model^\\text{teacher}$ is a powerful model (large number of parameters, lots of training)\n",
    "- $\\model^{\\text{student}}$ is smaller and untrained in the Target task\n",
    "\n",
    "Using Distillation to transfer skills from a large to a smaller model\n",
    "- results in very powerful small $\\model^{\\text{student}}$\n",
    "- that may not have had the capability to be *directly trained* in the skill\n",
    "- using only the training data used to train $\\model^\\text{teacher}$ in the skill\n",
    "    - Scaling Law: a model with fewer parameter needs more data to train than a model with more paramters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "DeepSeek-R1 is a reasoning model with 671 B parameters.\n",
    "    \n",
    "DeepSeek-R1 was used to create smaller distilled models\n",
    "- from small models\n",
    "    - of sizes: 1.5B, 7B, 8B, 14B, 32B, 70B)\n",
    "    - from the Llama and Qwen families of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**FYI**\n",
    "\n",
    "DeepSeek-R1 \n",
    "- derived from non-reasoning model DeepSeek-V3-Base of the same size\n",
    "- using a mixture of Reinforcement Learning (RL) and Supervised Fine Tuning (SFT)\n",
    "- Note\n",
    "    - both models are Mixture of Experts (MoE)\n",
    "    - only 37 B parameters are \"activated\" during inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "343px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
