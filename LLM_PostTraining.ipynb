{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\V}{\\mathbf{V}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\Reals}{{\\mathbb{R}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "\\newcommand{\\idxspatial}{{\\text{idx}}}\n",
       "\\newcommand{\\summaxact}{\\text{max}}\n",
       "\\newcommand{idxb}{\\mathbf{i}}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "% NLP\n",
       "\\newcommand{\\Vocab}{\\mathbf{V}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\offset}{o}\n",
       "\\newcommand{\\o}{o}\n",
       "\\newcommand{\\Emb}{\\mathbf{E}}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "%\n",
       "% LLM Reasoning\n",
       "\\newcommand{\\rat}{\\mathbf{r}}\n",
       "\\newcommand{\\model}{\\mathcal{M}}\n",
       "\\newcommand{\\bthink}{\\text{<think>}}\n",
       "\\newcommand{\\ethink}{\\text{</think>}}\n",
       "%\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "\\def\\OrderOf#1{\\mathcal{O}\\left( {#1} \\right)}\n",
       "%\n",
       "% Expectation operator\n",
       "\\def\\Exp#1{\\underset{#1} {\\operatorname{\\mathbb{E}}} }\n",
       "%\n",
       "% VAE\n",
       "\\def\\prs#1#2{\\mathcal{p}_{#2}(#1)}\n",
       "\\def\\qr#1{\\mathcal{q}(#1)}\n",
       "\\def\\qrs#1#2{\\mathcal{q}_{#2}(#1)}\n",
       "%\n",
       "% Reinforcement learning\n",
       "\\newcommand{\\Actions}{{\\mathcal{A}}} \n",
       "\\newcommand{\\actseq}{A}\n",
       "\\newcommand{\\act}{a}\n",
       "\\newcommand{\\States}{{\\mathcal{S}}}   \n",
       "\\newcommand{\\stateseq}{S}  \n",
       "\\newcommand{\\state}{s}\n",
       "\\newcommand{\\Rewards}{{\\mathcal{R}}}\n",
       "\\newcommand{\\rewseq}{R}\n",
       "\\newcommand{\\rew}{r}\n",
       "\\newcommand{\\transp}{P}\n",
       "\\newcommand{\\statevalfun}{v}\n",
       "\\newcommand{\\actvalfun}{q}\n",
       "\\newcommand{\\disc}{\\gamma}\n",
       "%\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Post-Training an LLM to create a  useful Assistant\n",
    "\n",
    "An LLM solves the Language Modeling task\n",
    "- predict the next output token\n",
    "- conditional on all the previous output tokens\n",
    "\n",
    "It learns to solve this task by extensive *Pre-Training*\n",
    "- the \"PT\" in \"GPT\"\n",
    "- using Supervised Learning\n",
    "    - examples: $\\langle \\text{prefix}, \\text{next token} \\rangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But a human would find it frustrating to use the LLM immediately after pre-training.\n",
    "\n",
    "A human wanting to know about the Black-Scholes equation\n",
    "- would need to formulate the request following the \"predict the next\" paradigm\n",
    "\n",
    "        The Black-Scholes equation states ...\n",
    "- rather than the more familiar \"question answering\" paradigm\n",
    "\n",
    "        What does the Black-Scholes equation state ?\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"raw\" LLM is transformed into a useful \"Assistant\" (e.g., ChatGPT)\n",
    "- by *Post-Training* the LLM.\n",
    "\n",
    "Post-training is a sequence of steps, where each step may impart\n",
    "- new knowledge\n",
    "- new behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples of Post-Training\n",
    "\n",
    "| Step Name                 | Description                                               | Methodology       | Illustration of Training Data                         |\n",
    "|:---------------------------|:-----------------------------------------------------------|:-------------------|:------------------------------------------------------|\n",
    "| Instruction Tuning        | Fine-tuning on instruction-response pairs to improve instruction following | SFT               | Input: \"Explain photosynthesis\"\\nOutput: \"Photosynthesis is...\" |\n",
    "| Domain Adaptation         | Specializing LLMs to a specific field (e.g., legal, medical) | SFT               | Input: Legal query\\nOutput: Legal-specific answer     |\n",
    "| RLHF (Reinforcement Learning from Human Feedback) | Optimizing model behavior based on human preference feedback | RL                | Reward signal from ranked model outputs               |\n",
    "| Direct Preference Optimization (DPO) | Similar to RLHF but optimizes directly from preferences without full RL loop | RL                | Preference pairs with win/loss signals                |\n",
    "| Safety Fine-Tuning        | Fine-tuning to reduce harmful or biased outputs         | SFT or RL (hybrid) | Labeled safe vs unsafe examples                        |\n",
    "| Retrieval-Augmented Generation | Integrating external knowledge retrieval at inference time | Not training, inference-time | Input with retrieved context + prompt                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From Generalist to Domain Specialist\n",
    "\n",
    "The Pre-Trained LLM is a Generalist trained on a vast array of knowledge.\n",
    "\n",
    "Turning the Generalist to a Specialist in some domain (e.g., Finance) by imparting domain-specific knowledge\n",
    "is quite useful\n",
    "- Finance Assistant\n",
    "- Medical Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aligning the model with Human Intent\n",
    "\n",
    "The Pre-Trained LLM still expects input in a form appropriate for Next Token prediction.\n",
    "\n",
    "This is not natural for a human User.\n",
    "\n",
    "We can post-train the LLM with the following behaviors\n",
    "- *Instruction following*\n",
    "    - interpret the human input as a request to follow instructions/answer questions\n",
    "- *Chat*\n",
    "    - engage in a multi-turn *conversation* between User and Assistant\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Human intent may also involve other desirable characteristics for the Assistant's responses\n",
    "\n",
    "Post-training for the following skills restricts the Assistant's responses to \"desirable\" behavior\n",
    "- helpful\n",
    "- honest\n",
    "- harmless: absence of\n",
    "    - toxicity\n",
    "    - bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tool usage\n",
    "\n",
    "The Pre-Trained LLM's \"knowledge\" is limited to that which is acquired in training\n",
    "- it knows nothing about current events\n",
    "\n",
    "Similarly, the \"predict the next\" paradigm results in poor mathematical ability (e.g. adding two numbers).\n",
    "\n",
    "We allow the LLM to access *tools*\n",
    "- Web browser\n",
    "- Calculator\n",
    "- Python\n",
    "\n",
    "in order to extend its skills.\n",
    "\n",
    "This requires post-training in *Tool Usage*\n",
    "- learn which tool to use; when to use it; how to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Post-Train an LLM\n",
    "\n",
    "There are two primary methods for post-training\n",
    "- Supervised Fine-Tuning (SFT)\n",
    "    - continuation of Pre-Training using Predict the Next paradigm\n",
    "    - learn by minimizing a Loss\n",
    "- Reinforcement Fine Tuning (RFT)\n",
    "    - modify behavior by Reinforcement Learning\n",
    "    - learn by maximizing \"rewards\"\n",
    "\n",
    "We have separate modules on SFT and RL.\n",
    "\n",
    "Here we will focus  specifically on Reinforcement Fine Tuning (RFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Technique            | Definition                                                                      | Use Case Examples                        | Relative Generalization |\n",
    "|:----------------------|:--------------------------------------------------------------------------------|:------------------------------------------|------------------------|\n",
    "| SFT                  | Fine-tuning with labeled (input, output) pairs                                 | Chatbots, summarization, code generation | Lower                  |\n",
    "| RL (e.g., RLHF, DPO) | Optimizing model behavior with reward/preference feedback signals (often human-led) | Alignment, safety, general dialogue      | Higher                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Fine Tuning\n",
    "\n",
    "There are several specific uses of RFT that have been used to\n",
    "align a Pre-Trained LLM with Human Intent.\n",
    "\n",
    "We illustrate\n",
    "- Reinforcement Learning with Human Feedback (RLHF)\n",
    "- Reinforcement Learning with AI Feedback (RLAIF)\n",
    "- Constitutional AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Aspect              | RLHF                               | RLAIF                              | Constitutional AI                      |\n",
    "|:--------------------|:----------------------------------|:----------------------------------|:-------------------------------------|\n",
    "| Feedback source     | Human annotators                   | AI or human feedback interactively| AI model itself following Constitution|\n",
    "| Data generation     | Human-labeled preferences          | Interactive AI feedback with rewards | Synthetic preference & revision dataset|\n",
    "| Scalability         | Limited by human resources         | More scalable                      | More scalable via AI-generated data   |\n",
    "| Goal                | Align model to human values        | Align model with interactive feedback| Align model with AI-defined principles|\n",
    "| Methodology         | Reinforcement learning guided by human feedback | RL guided by AI or human in-the-loop feedback| RL with AI feedback based on fixed Constitution|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we begin, we first show \n",
    "- how to cast an LLM following the Language Modeling objective\n",
    "- into a form more familiar to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Probability of a Trajectory $\\pi_\\theta (y | x)$ when $y$ is the output of an Auto-regressive LLM\n",
    "\n",
    "LLM's produce outputs that are sequences of tokens, according to the \"policy\" of the LLM model.\n",
    "- the policy\n",
    "$$\n",
    "\\pi_\\theta( y_\\tt | y_{[0:\\tt-1] } )\n",
    "$$\n",
    "\n",
    "- defines a probability distribution over the tokens in the Vocabulary\n",
    "\n",
    "where\n",
    "- $y$ is the output sequence\n",
    "- $y_\\tt$ is the element of the sequence at position $\\tt$\n",
    "- $y_{[0:\\tt-1] }$ is the prefix of $y$ ending at position $\\tt-1$\n",
    "\n",
    "This \"next token prediction\" policy is the Language Modeling Objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Chaining together the conditional probabilities of each element in the sequence gives the probability of the sequence $y$\n",
    "\n",
    "$$\n",
    "\\pi_\\theta( y ) = \\prod_{\\tt=1}^T { \\pi( y_\\tt | \\y_{[0:\\tt-1]} ) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can view the behavior of an LLM engaged in Language Modeling through the lens of RL:\n",
    "\n",
    "- there is a state $\\state$ corresponding to the partial output (prefix of $y$) $y_{[0:\\tt-1]}$\n",
    "- An action $\\act \\in \\Actions$, is the output of one token from the Vocabulary\n",
    "- the LLM implements a policy producing the next token, conditional on the state (prefix)\n",
    "$$\\pi(\\act | \\state)$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\pi (  y_\\tt | y_{[0:\\tt-1]} )\n",
    "$$\n",
    "\n",
    "is the policy probability of \n",
    "- taking the action: output $y_\\tt$ as the next token\n",
    "- conditional being in state/having output  $$y_{[0:\\tt-1]}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning with Human Feedback (RLHF)\n",
    "\n",
    "Let use consider our LLM as following a \"policy\"\n",
    "- The Policy Model in RL language\n",
    "\n",
    "\n",
    "An idealized workflow for Alignment interjects a human in the training as follows\n",
    "- A prompt is chosen from training data\n",
    "- The prompt is fed to the agent/Policy Model  in order to generate a response\n",
    "    - the prompt is sometimes called the *context*\n",
    "- Human evaluates the desirability of the response\n",
    "- Agent modifies its parameters based on the human's feedback\n",
    "\n",
    "This describes *Reinforcement Learning with Human Feedback*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Reinforcement Learning with Human Feedback</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <img src=\"images/instruct_gpt_process.png\" width=75%>\n",
    "    </tr>      \n",
    "    <tr>\n",
    "    Source: https://openai.com/blog/instruction-following/#methods\n",
    "    </tr>\n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The three steps in RLHF shown in the diagram\n",
    "- SFT to demonstrate the behavior\n",
    "- Creating a Reward Model\n",
    "- RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Fine Tuning\n",
    "\n",
    "The desired behavior is demonstrated by\n",
    "- human written demonstrations\n",
    "    - prompt/response pairs\n",
    "\n",
    "SFT is used to create a \"baseline\"\n",
    "- LLM with primitive, narrow implementation of the behavior\n",
    "- solves the \"cold start\" problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a Reward model\n",
    "\n",
    "Reinforcement Learning is doing the \"heavy lifting\" of imparting the desired behavior.\n",
    "\n",
    "But RL requires a Reward Model\n",
    "- provide feedback as learning signals to train the model in the desired behavior\n",
    "\n",
    "First we observe that it is sufficient for the rewards to be\n",
    "- trajectory rewards for the entire response\n",
    "- rather than per-step rewards\n",
    "    - reward for the action of generating the next token of the response\n",
    "\n",
    "Where do these rewards come from ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Human-generated rewards**\n",
    "\n",
    "We start by asking humans to create rewards\n",
    "- given a prompt/response pair: assign a reward\n",
    "\n",
    "But asking a human to create a scalar reward is problematic\n",
    "- two different humans may have different \"scales\"\n",
    "    - \"good\" for Human A may be 95%; for Human B it may be 75%\n",
    "- the same human may not be consistent in assigning rewards across different prompts\n",
    "    - good may be 95% on one prompt and 90% on a different prompt\n",
    "    \n",
    "In general, asking a human to provide scalar reward leads to inconsistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Humans are far more consistent when asked to *rank* potential responses\n",
    "- order rather than magnitude\n",
    "\n",
    "Given prompt $x$ and two outputs to the prompt\n",
    "- the human ranks the two outputs\n",
    "- creating an example of *Preference Data*\n",
    "\n",
    "    $$(x, y^+, y^-)$$\n",
    "    \n",
    "where response $y^+$ is preferred over response $y^-$\n",
    "\n",
    "Preference Data is likely to be more consistent, when generated by a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**AI-generate reward**\n",
    "\n",
    "Having a human-in-the-loop for training is not practical.\n",
    "\n",
    "The solution is to *train a reward model*\n",
    "- a NN that learns human preferences\n",
    "- from a small collection of human-labeled Preference Data\n",
    "\n",
    "The Reward Model is now a replacement for the Human\n",
    "- and the source of Trajectory rewards for RL Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "With the Reward Model in hand, we can apply Reinforcement Learning\n",
    "- using *only* a set or prompts/questions as input\n",
    "    - no need for a labeled \"correct\" response\n",
    "    - RL vs Supervised Learning\n",
    "    \n",
    "The flow is\n",
    "- select a prompt $x$ as input\n",
    "- use the LLM/Policy Model to generate\n",
    "    - one or more responses\n",
    "- use the Reward Model to rank the responses\n",
    "- use RL on these reward to modify the Policy Model\n",
    "    - to increase likelihood of outputting the preferred response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning with AI Feedback (RLAIF)\n",
    "\n",
    "The Human Feedback (HF) of RLHF occurs in Step 2\n",
    "- For each prompt\n",
    "    - the LLM generates multiple responses\n",
    "    - human ranks the responses\n",
    "\n",
    "The ranked responses create a Preference Data dataset with which to train the Reward Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "in *Reinforcement Learning with AI Feedback (RLAIF)*\n",
    "- we replace the human\n",
    "- with a LLM\n",
    "\n",
    "The LLM performs the ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>Reinforcement Learning with Human Feedback/AI Feedback</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <img src=\"images/RLAIF_vs_RLHF.png\" width=75%>\n",
    "    </tr>      \n",
    "    <tr>\n",
    "    Source: https://arxiv.org/pdf/2309.00267#page=2\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can even take this one step further\n",
    "- have the LLM generate the prompt\n",
    "- and then have it produce multiple responses\n",
    "- which it then ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constitutional AI\n",
    "\n",
    "Constitutional AI is a hybrid of RLHF and RLAIF\n",
    "- it follows the workflow of RLHF\n",
    "- but replaces the human as much as possible\n",
    "    - creation of the training data for SFT (Step 1)\n",
    "    - creation of Preference Data for RL (Step 3)\n",
    "\n",
    "\n",
    "What is novel is how the multiple responses are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A *Constitution* is a list of principles for desired behavior.\n",
    "\n",
    "For example, here is a Constitution to guide an Assistant to be Helpful, Honest and avoid Harm.\n",
    "\n",
    "| Principle          | Description                                                                                   |\n",
    "|:-------------------|:---------------------------------------------------------------------------------------------|\n",
    "| Be helpful         | Always aim to assist users clearly and effectively with their requests. Provide relevant information. |\n",
    "| Avoid harm         | Do not generate harmful, offensive, or unsafe content. Avoid promoting violence, hate, or misinformation. |\n",
    "| Be honest          | Provide accurate and truthful information. Avoid fabrications or misleading statements.       |\n",
    "| Respect privacy    | Do not reveal or infer sensitive personal information.                                       |\n",
    "| Maintain neutrality| Remain unbiased and impartial, avoiding political or ideological slants.                      |\n",
    "| Clarify uncertainty| Honestly acknowledge limits of knowledge instead of guessing when unsure.                     |\n",
    "| Encourage positivity| Foster constructive and respectful communication.                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simplified Constitutional AI workflow**\n",
    "\n",
    "The base LLM is first prompted with a question $x$\n",
    "- producing responses $y$.\n",
    "\n",
    "$x$ is augmented with language that seeks to provoke a response $y$ that violates the desired behavior\n",
    "- violates the Constitution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example from the paper.  The initial context (prompt) is provided by \"Human\" and the response begins \"Assistant: \"\n",
    "\n",
    "    Human: Can you help me hack into my neighbor’s wifi?\n",
    "    Assistant: Sure thing, you can use an app called VeryEasyHack that will allow\n",
    "    you to log in to your neighbor’s wifi.\n",
    "\n",
    "The initial context has the Critique Request appended, and the \"Assistant\" responds\n",
    "\n",
    "    Critique Request: Identify specific ways in which the assistant’s last response\n",
    "    is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\n",
    "    \n",
    "    Critique: The assistant’s last response is harmful because hacking into someone\n",
    "    else’s wifi is an invasion of their privacy and is possibly illegal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The base LLM is then asked\n",
    "- to select a principle from the Constitution\n",
    "- and critique response $y$ in light of the chosen principle\n",
    "\n",
    "Based on the critique\n",
    "- it is asked to generate an alternate response $y'$ that addresses the critique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Revision Request is appended to the context and the \"Assistant\" responds\n",
    "\n",
    "    Revision Request: Please rewrite the assistant response to remove any and all\n",
    "    harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "    \n",
    "    Revision: Hacking into your neighbor’s wifi is an invasion of their privacy, and\n",
    "    I strongly advise against it. It may also land you in legal trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This implicitly creates the preference triple\n",
    "$$\n",
    "(x, y^+, y^-)\n",
    "$$\n",
    "where\n",
    "- $y^- = y$\n",
    "- $y^+ = y'$\n",
    "\n",
    "That can be used for RL with Preference Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**General Constitutional AI workflow**\n",
    "\n",
    "The simplified workflow still requires a human\n",
    "- to create the training data for the SFT (Step 1)\n",
    "    - which instills basic good behavior in the base LLM\n",
    "    - prior to RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the completely general workflow, the base LLM\n",
    "- is prompted with $x$ and provoked into producing an undesirable response $y$\n",
    "- is asked to critique and produce an improved response $y'$\n",
    "\n",
    "This is done for multiple prompts\n",
    "- creating labeled examples $\\langle x, y' \\rangle$\n",
    "- which is used as the dataset for the SFT step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This bootstrapping instills basic good behavior into the base LLM\n",
    "- creating an LLM called SL-CAI \n",
    "    - *Supervised Learning - Constitutional AI*\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SL-CAI model is then\n",
    "- prompted with $x$\n",
    "- and asked to produce multiple responses\n",
    "- and to rank them\n",
    "- in order to create Preference Data with which to train a Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "Technically, the ranking is performed\n",
    "- By creating a multiple choice question \n",
    "    - with body equal to $x$\n",
    "    - and choices $y, y', ...$ generated by the SL-CAI\n",
    "- the log probabilities of each choice is used as a proxy for ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is the template:\n",
    "\n",
    "    Consider the following conversation between a human and an assistant:\n",
    "    [HUMAN/ASSISTANT CONVERSATION]\n",
    "    [PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
    "    Options:\n",
    "    (A) [RESPONSE A]\n",
    "    (B) [RESPONSE B]\n",
    "    The answer is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case study: Post-training a model to reason\n",
    "\n",
    "A *reasoning model* provides responses with a distinctive style\n",
    "- format\n",
    "    - *long* Chain of Thought (CoT): step-by-step reasoning\n",
    "- process\n",
    "    - *reflection*: looking back at the response so far, and evaluating the solution and strategy\n",
    "    - *revision*: adapting/changing the current response and strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Reasoning Format example**\n",
    "\n",
    "**Instruction:**  \n",
    "\"Explain step-by-step how to find the greatest common divisor (GCD) of 48 and 18.\"\n",
    "\n",
    "**Expected Reasoning Format:**  \n",
    "1. **State the problem clearly:** \"We want to find the GCD of 48 and 18.\"  \n",
    "2. **Describe the method or approach:** \"We will use the Euclidean algorithm.\"  \n",
    "3. **Stepwise execution:**  \n",
    "   - Step 1: Divide 48 by 18, the remainder is 12.  \n",
    "   - Step 2: Divide 18 by 12, the remainder is 6.  \n",
    "   - Step 3: Divide 12 by 6, the remainder is 0.  \n",
    "4. **Conclusion:** \"Since the remainder is now 0, the GCD is the last non-zero remainder, which is 6.\"\n",
    "\n",
    "**Formatted Output:**  \n",
    "\"We want to find the GCD of 48 and 18. Using the Euclidean algorithm,  \n",
    "Step 1: 48 divided by 18 leaves a remainder of 12.  \n",
    "Step 2: 18 divided by 12 leaves a remainder of 6.  \n",
    "Step 3: 12 divided by 6 leaves a remainder of 0.  \n",
    "Therefore, the GCD of 48 and 18 is 6.\"\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning behavior is something that is instilled in post-training\n",
    "- Not the natural behavior of an LLM or Assistant\n",
    "\n",
    "We will demonstrate how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use Reinforcement Fine Tuning (RFT).\n",
    "\n",
    "As you may have noticed in our previous section, RFT has at least two steps\n",
    "- Supervised Fine Tuning\n",
    "- Reinforcement Learning\n",
    "    - usually with Preference Data\n",
    "\n",
    "We will review each step and explain why they are both necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Fine Tuning + Reinforcement Learning: Why both ?\n",
    "\n",
    "It may seem confusing to need both SFT and RL.\n",
    "\n",
    "Very loosely\n",
    "- SFT is used to teach the base model the *style* of a reasoning response\n",
    "    - syntax\n",
    "    - surface level\n",
    "- RL is used to ensure that the reasoning response behaves according to *valid logic*\n",
    "    - semantic\n",
    "    - deeper level\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Supervised Fine Tuning is more about\n",
    "- *imitating* training examples\n",
    "   - can *overfit* to training examples\n",
    "       - it is the nature of the Loss function\n",
    "        - bounded below by $0$ \n",
    "- than *understanding* a process \n",
    "   - fail to generalize beyond training examples\n",
    " \n",
    "Reinforcement Learning creates a deeper understanding\n",
    "- iterative feedback via rewards\n",
    "    - maximizing return: can always try to improve\n",
    "    - no clear upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On a more practical level, to instill reasoning behavior\n",
    "- SFT \n",
    "    - requires *many* training examples\n",
    "    - typically: human labeled\n",
    "- RL \n",
    "    - needs fewer examples\n",
    "    - iterative improvement with each reward\n",
    "    - can *re-use* the same example to improve further\n",
    "        - reward can increase with each re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Fine Tuning: avoiding the \"cold start\" problem\n",
    "\n",
    "It would seem that RL is superior to SFT\n",
    "- why is SFT necessary ?\n",
    "\n",
    "A partial answer is that\n",
    "- reasoning responses\n",
    "- are *very different* than the response to the same prompt on a base model\n",
    "    - it is \"out of distribution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reinforcement Learning struggles with the \"out of distribution\" responses of the training examples\n",
    "- Sparse rewards\n",
    "    - trajectory reward, no intermediate reward\n",
    "    \n",
    "SFT is very good at adapting the base model's outputs to the \"new distribution\"\n",
    "- the different style of a reasoning response\n",
    "\n",
    "Hence SFT is usually used as an initial step.\n",
    "\n",
    "This overcomes the *Cold Start* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interestingly, SFT instills\n",
    "- the *format*\n",
    "    - step by step\n",
    "- and *patterns*\n",
    "    - reflection, revision\n",
    "- *not necessarily* correctness of reasoning !\n",
    "    - or at least: correct w.r.t. training examples\n",
    "    - poor generalization\n",
    "    \n",
    "SFT creates a stable base upon which RL can learn to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Stage           | Purpose in Reasoning Induction                                  | Training Signal/Data                                   | Strengths                              | Limitations                          |\n",
    "|:----------------|:---------------------------------------------------------------|:-------------------------------------------------------|:--------------------------------------|:-------------------------------------|\n",
    "| SFT             | Learn reasoning formats and step-by-step logic                  | Paired (instruction, reasoning chain) examples         | Provides stable, structured output     | Limited generalization, mimicry       |\n",
    "| RL (e.g., RLHF) | Refine reasoning quality, encourage adaptive, genuine reasoning | Reward signals based on output quality or preferences  | Improves correctness and flexibility   | Requires strong warm-start (SFT)      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References for SFT and RL stages**\n",
    "\n",
    "- [SFT or RL? An Early Investigation into Training R1-Like Reasoning Models](https://arxiv.org/html/2504.11468v1)\n",
    "- [Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning](https://arxiv.org/html/2506.04723v1)\n",
    "- [Beyond Next-Token Prediction: How Post-Training Teaches LLMs to Reason](https://toloka.ai/blog/how-post-training-teaches-llms-to-reason/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DeepSeek: investigating the Cold Start problem\n",
    "\n",
    "DeepSeek-R1 is a well known reasoning model.\n",
    "\n",
    "Its development included experiments centered around the necessity of the SFT step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Specifically\n",
    "- the authors tried an *RL only* (no SFT) approach\n",
    "- resulting in a reasoning model DeepSeek-R1-Zero\n",
    "    - strong reasoning\n",
    "    - inconsistent formatting\n",
    "        - mixed English/Chinese output !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This confirmed the need for\n",
    "- at least a *small* number of training examples for SFT\n",
    "- to overcome the Cold Start\n",
    "\n",
    "**But** the inconsistent DeepSeek-R1-Zero was still very useful\n",
    "- was prompted to create reasoning responses\n",
    "- these inconsistent reasoning \n",
    "- were filtered/curated by the human developers to overcome format issues\n",
    "- in order to create an *expanded set* of SFT examples !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This bootstrapping resulted in a large SFT training set\n",
    "- 600K examples\n",
    "- which improved the SFT step greatly\n",
    "    - adherence to format\n",
    "    - instruction-following\n",
    "- but the SFT-only (i.e., without the subsequent RL step) model\n",
    "    - still failed to \n",
    "        - reason correctly\n",
    "        - generalize out of sample\n",
    "\n",
    "This validate the necessity of the RL step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Synthetic generation of reasoning examples \n",
    "\n",
    "Using this idea, we can fine-tune (via SFT) a base model\n",
    "- to produce responses in reasoning **format**\n",
    "- not necessarily logically correct\n",
    "- not necessarily using the right process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This fine-tuned model becomes\n",
    "- an abundant source of training examples\n",
    "- for the SFT step of RFT\n",
    "\n",
    "**Note**\n",
    "\n",
    "Distinguish between \n",
    "- the SFT-model trained to produce examples for RFT\n",
    "- the base model that uses these examples for the initial SFT step of RFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a hypothetical one-shot prompt\n",
    "- to create a new example of a question\n",
    "- and a reasoning response\n",
    "\n",
    "Its goal is to tune the model to\n",
    "- produce responses\n",
    "- in the desired format\n",
    "    - structured sections for major steps\n",
    "    - step by step answer strategy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**One-shot prompt**\n",
    "\n",
    "    Below is an example showing how to answer a question with clear structured reasoning including labeled sections.\n",
    "\n",
    "    For each new question you invent, provide the reasoning answer in the same labeled format.\n",
    "\n",
    "    **Instruction:**  \n",
    "    \"Explain step-by-step how to find the greatest common divisor (GCD) of 48 and 18.\"\n",
    "\n",
    "    **Expected Reasoning Format:**  \n",
    "    1. **State the problem clearly:** \"We want to find the GCD of 48 and 18.\"  \n",
    "    2. **Describe the method or approach:** \"We will use the Euclidean algorithm.\"  \n",
    "    3. **Stepwise execution:**  \n",
    "       - Step 1: Divide 48 by 18, the remainder is 12.  \n",
    "       - Step 2: Divide 18 by 12, the remainder is 6.  \n",
    "       - Step 3: Divide 12 by 6, the remainder is 0.  \n",
    "    4. **Conclusion:** \"Since the remainder is now 0, the GCD is the last non-zero remainder, which is 6.\"\n",
    "\n",
    "    **Formatted Output:**  \n",
    "    \"We want to find the GCD of 48 and 18. Using the Euclidean algorithm,  \n",
    "    Step 1: 48 divided by 18 leaves a remainder of 12.  \n",
    "    Step 2: 18 divided by 12 leaves a remainder of 6.  \n",
    "    Step 3: 12 divided by 6 leaves a remainder of 0.  \n",
    "    Therefore, the GCD of 48 and 18 is 6.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Reference to DeepSeek-R1**\n",
    "\n",
    "[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning from Diverse Feedback](https://arxiv.org/abs/2501.12948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: RFT = SFT + RL with Preference Data\n",
    "\n",
    "To make all the steps clear, we provide an example of each step (and sub-step in some cases).\n",
    "\n",
    "These examples reflect how \n",
    "- SFT focuses on getting the model to produce structured, formatted reasoning \n",
    "    - by learning from labeled examples\n",
    "- RL uses reward feedback\n",
    "    - to push the model\n",
    "    - toward more accurate, meaningful, and high-quality reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: SFT Training Example (Instruction + Detailed Reasoning)**\n",
    "\n",
    "Here is an input example used in the SFT step\n",
    "- to train the base model to produce the response in **correct format**\n",
    "\n",
    "**Input (Instruction + Question):**  \n",
    "\"Explain step-by-step how to solve the equation 2x + 3 = 9.\"\n",
    "\n",
    "**Output (Reasoning Steps):**  \n",
    "\"Step 1: Subtract 3 from both sides: 2x + 3 - 3 = 9 - 3, which simplifies to 2x = 6.  \n",
    "Step 2: Divide both sides by 2: 2x / 2 = 6 / 2, so x = 3.\"\n",
    "\n",
    "*Note:* This example teaches the **format and structure** of reasoning—how to break a problem down into clear steps.\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the SFT can produce outputs\n",
    "- in the correct format\n",
    "- but with flawed logic\n",
    "    - RL instills correct logical reasoning\n",
    "    \n",
    "For example:\n",
    "\n",
    "**Input:**  \n",
    "\"Explain why the Earth revolves around the Sun.\"\n",
    "\n",
    "**Output:**  \n",
    "\"The Earth moves around the Sun because the Sun is bigger and pulls the Earth with its big gravity.\"\n",
    "\n",
    "*Note:* The format is a coherent explanation, but the reasoning may be oversimplified or imprecise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: RL Training Data Example (Preferences/Rewards)**\n",
    "\n",
    "Here is an example of the input to **train the reward/preference model**\n",
    "\n",
    "The trained model can then be used to create a Preference Data set for the RL step.\n",
    "\n",
    "**Candidate Outputs for the same input:**\n",
    "\n",
    "- **Output A:**  \n",
    "\"Step 1: Subtract 3 from both sides: 2x = 6. Step 2: Divide both sides by 2: x = 3.\"  \n",
    "(Concise and logically correct.)\n",
    "\n",
    "- **Output B:**  \n",
    "\"Subtract 3 from both sides and divide by 2, so x = 3. This is because math.\"  \n",
    "(Vague and incomplete reasoning.)\n",
    "\n",
    "**Reward Signal:**  \n",
    "Output A is *preferred* and given a higher reward; Output B is penalized for lack of detailed and correct reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Example: RL Encouraging Improved Reasoning Quality**\n",
    "\n",
    "Here is an example of the input to the RL step\n",
    "- an example of Preference Data\n",
    "- where the reward for the preferred responses is higher than for the non-preferred response\n",
    "\n",
    "**Candidate Outputs:**\n",
    "\n",
    "- **Output A:**  \n",
    "\"The Earth revolves around the Sun due to the gravitational force described by Newton's law of universal gravitation, where the Sun's mass exerts a force on Earth keeping it in orbit.\"\n",
    "\n",
    "- **Output B:**  \n",
    "\"The Sun is big and bright, so Earth moves around it.\"\n",
    "\n",
    "**Reward:**  \n",
    "Output A receives higher reward for scientifically accurate and logically sound reasoning, refining the correctness beyond SFT’s imitation.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison: SFT, RL, RFT\n",
    "\n",
    "\n",
    "SFT and RL are different methods for fine tuning an LLM.\n",
    "- RFT combines an initial SFT with a subsequent RL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The distinction becomes every blurrier\n",
    "- when RL has intermediate rewards\n",
    "- rather than a single trajectory reward\n",
    "\n",
    "It is sometimes possible to cast a task into a form appropriate for either SFT or RL with per-step rewards\n",
    "- Next token prediction\n",
    "    - SFT: Cross Entropy Loss for every step\n",
    "    - RL: Per-step reward\n",
    "        - +1 reward for correct prediction/0 for incorrect prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the choice of which method to use is often dependent on\n",
    "- the task\n",
    "- the available training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is hard to be precise, but here are some thematic comparisons.\n",
    "\n",
    "SFT \n",
    "- encourages imitation of  the label of an example\n",
    "    - exact match\n",
    "- enforces formatting/structure of response\n",
    "- \"surface\" level correctness\n",
    "- well-suited to precisely-defined tasks\n",
    "    - with *objective* measures of success\n",
    "    - quantitative measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RL \n",
    "- allows multiple \"correct\" answers\n",
    "    - which may be ranked\n",
    "- \"deeper\" understanding/generalization\n",
    "- well-suited to more loosely-defined tasks\n",
    "    - with *subjective* measures of success\n",
    "    - *qualitative* measures\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In terms of training data\n",
    "- SFT imitation requires *lots* of training examples\n",
    "    - exploration of alternatives doesn't come into play\n",
    "- RL can often be accomplished in a very small number of training examples\n",
    "    - exploration encourage\n",
    "    \n",
    "SFT and RL are *complementary* methods for fine tuning an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use of SFT as first phase of RFT\n",
    "\n",
    "The preliminary SFT phase of RFT serves several purposes\n",
    "- move the LLM's distribution to the *format* of tasks for RL to learn\n",
    "    - primes the RL phase with correctly formated examples\n",
    "- creates examples with different rewards\n",
    "    - RL learns from the *contrasts* between high and low rewards\n",
    "- \"bootstrap\" the RL training dataset\n",
    "    - uses iterative SFT\n",
    "        - uses a weaker SFT-tuned model\n",
    "        - to create synthetic training examples for a stronger SFT-tuned model\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Criteria                 | SFT                                                        | RFT/RLHF                                             |\n",
    "|:-------------------------|:-----------------------------------------------------------|:-----------------------------------------------------|\n",
    "| Task type                | Objective, well-defined, clear correct answer tasks         | Subjective, ambiguous, or value-laden tasks          |\n",
    "| Data availability        | Large, high-quality labeled datasets available              | Little/no labeled data, but feedback/preference signals are available |\n",
    "| Training complexity      | Simpler (labeled pairs)                                    | More complex (reward model, RL optimization)         |\n",
    "| Desired outcome          | Accuracy, task performance, factual correctness             | Human preference alignment, style, quality, safety   |\n",
    "| Overfitting risk         | Higher, if data is limited                                 | Lower; learns general behavior from rewards          |\n",
    "| Generalization           | Prone to memorization                                      | Promotes adaptability, nuanced behaviors             |\n",
    "| Cost/resource needs      | Lower; less human-in-the-loop need                         | Higher; human feedback collection and more computation|\n",
    "| Ideal use cases          | Translation, classification, summarization, retrieval      | Chatbots, open-domain QA, content moderation, dialog |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is one rubric:\n",
    "\n",
    "<table>\n",
    "<img src=\"images/rft_vs_sft_decision.png\" width=90%>\n",
    "     \n",
    " Reference: https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce\n",
    "<table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**References for RFT vs SFT**\n",
    "\n",
    "- [Why Reinforcement Learning Beats SFT with Limited Data - Predibase](https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce)\n",
    "- [Preference Alignment vs Supervised Fine-Tuning in LLM Training](https://www.rohan-paul.com/p/preference-alignment-vs-supervised)\n",
    "- [Supervised Fine-Tuning vs. RLHF: How to Choose the Right Approach](https://www.invisible.co/blog/supervised-fine-tuning-vs-rlhf-how-to-choose-the-right-approach-to-train-your-llm)\n",
    "- [Fine-Tuning vs RLHF: Choosing the Best LLM Training Method](https://cleverx.com/blog/supervised-fine-tuning-vs-rlhf-choosing-the-right-path-to-train-your-llm)\n",
    "- [What is supervised fine-tuning? - BlueDot Impact](https://bluedot.org/blog/what-is-supervised-fine-tuning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Process Reward Model (PRM) vs Outcome Reward Model (ORM)\n",
    "\n",
    "Outcome Reward Model (ORM) = Trajectory Reward\n",
    "- single reward at end of trajectory\n",
    "\n",
    "Process Reward Model (PRM) = step by step reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References for Process Reward Models vs Outcome Reward Models**\n",
    "\n",
    "- [A Comprehensive Survey of Reward Models: Taxonomy and Applications](https://arxiv.org/html/2504.12328v1)\n",
    "- [Reward Modeling | RLHF Book by Nathan Lambert](https://rlhfbook.com/c/07-reward-models.html)\n",
    "- [Let’s Verify Step by Step (OpenAI, Process Supervision)](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf)\n",
    "- [Getting LLMs To Reason With Process Rewards](https://patmcguinness.substack.com/p/getting-llms-to-reason-with-process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "| Title (linked)                                                                                                                           | Commentary                                                                                                                                                          |\n",
    "|:-----------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [InstructGPT: Aligning Language Models with Human Intent via RLHF](https://arxiv.org/abs/2203.02155)                                      | Foundational paper laying out the RLHF approach to align LLMs with human intent using human preference data. Essential for understanding RLHF theory and practice. |\n",
    "| [A Survey on Post-Training of Large Language Models](https://arxiv.org/abs/2503.06072)                                                    | Comprehensive survey reviewing SFT, RLHF, and newer alignment methods. Synthesizes research trends and challenges in LLM post-training.                          |\n",
    "| [Reinforcement Learning from AI Feedback (RLAIF): A Scalable Alternative to RLHF](https://arxiv.org/abs/2309.00267)                      | Introduces RLAIF, replacing human feedback with AI-generated feedback for scalable alignment. Critical for understanding automated feedback approaches.           |\n",
    "| [Constitutional AI: Harmlessness from AI Feedback](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)   | Proposes Constitutional AI, using a fixed ethical constitution for AI self-critique and revision to improve alignment without human labels.                       |\n",
    "| [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/abs/2502.21321)                                   | Examines post-training methods focused on improving reasoning in LLMs via SFT and RL, analyzing mechanics and challenges.                                         |\n",
    "| [SFT Memorizes, RL Generalizes: A Comparative Study of Post-Training Methods for LLMs](https://arxiv.org/abs/2501.17161)                  | Empirically compares SFT and RL in LLMs, showing SFT excels at memorization while RL generalizes better and improves alignment.                                    |\n",
    "| [How Reinforcement Learning Beats Supervised Fine-Tuning When Data Is Scarce](https://predibase.com/blog/how-reinforcement-learning-beats-supervised-fine-tuning-when-data-is-scarce) | Blog explaining why RL methods can outperform SFT in low-data regimes; offers practical insights for training efficiency.                                        |\n",
    "| [Beyond Next-Token Prediction: How Post-Training Teaches LLMs to Reason](https://toloka.ai/blog/how-post-training-teaches-llms-to-reason/) | Discusses how combining SFT and RL post-training enables complex reasoning in LLMs, with examples and experimental findings.                                      |\n",
    "| [Demystifying Reasoning Models](https://cameronrwolfe.substack.com/p/demystifying-reasoning-models)                                       | Blog unpacking the roles of SFT and RL in reasoning capability development; bridges theory and practice with clear explanations.                                  |\n",
    "| [RLHF vs RLAIF: A Detailed Comparison of AI Training Methods](https://www.sapien.io/blog/rlaif-vs-rlhf-understanding-the-differences)     | Detailed comparison of RLHF and RLAIF approaches, illustrating differences in feedback sources and workflows for AI alignment.                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9 (new)",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
